{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "vae_10m.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhQ8g-UDLpqn",
        "outputId": "361b4abd-778a-4be8-88a9-20421b8ca7e0"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y-MU2H5KbIZ"
      },
      "source": [
        "# Variational autoencoders for collaborative filtering "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBvM4FolKbIZ"
      },
      "source": [
        "This notebook accompanies the paper \"*Variational autoencoders for collaborative filtering*\" by Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara, in The Web Conference (aka WWW) 2018.\n",
        "\n",
        "In this notebook, we will show a complete self-contained example of training a variational autoencoder (as well as a denoising autoencoder) with multinomial likelihood (described in the paper) on the public Movielens-20M dataset, including both data preprocessing and model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqHVSXvVKbIa"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sn\n",
        "sn.set()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.layers import apply_regularization, l2_regularizer\n",
        "import bottleneck as bn\n",
        "from google.colab import drive"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jSKlwM2KbIa"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FfmcWszKbIa"
      },
      "source": [
        "We load the data and create train/validation/test splits following strong generalization: \n",
        "\n",
        "- We split all users into training/validation/test sets. \n",
        "\n",
        "- We train models using the entire click history of the training users. \n",
        "\n",
        "- To evaluate, we take part of the click history from held-out (validation and test) users to learn the necessary user-level representations for the model and then compute metrics by looking at how well the model ranks the rest of the unseen click history from the held-out users."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6aQkgc3KbIa",
        "outputId": "c253429d-0f2c-47e1-bbce-a8241f4eee70"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "### change `DATA_DIR` to the location where movielens-20m dataset sits\n",
        "DATA_DIR = '/content/drive/My Drive/Study/Project3/ml-10m'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qBQWbvSKbIa"
      },
      "source": [
        "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'ratings.csv'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq7xOWy9KbIa"
      },
      "source": [
        "# binarize the data (only keep ratings >= 4)\n",
        "raw_data = raw_data[raw_data['rating'] > 3.5]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "C_YwUsZwKbIa",
        "outputId": "8fb1e666-862e-4178-82c1-c199beff6fae"
      },
      "source": [
        "raw_data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>122</td>\n",
              "      <td>5.0</td>\n",
              "      <td>838985046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>185</td>\n",
              "      <td>5.0</td>\n",
              "      <td>838983525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>231</td>\n",
              "      <td>5.0</td>\n",
              "      <td>838983392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>292</td>\n",
              "      <td>5.0</td>\n",
              "      <td>838983421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>316</td>\n",
              "      <td>5.0</td>\n",
              "      <td>838983392</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   userId  movieId  rating  timestamp\n",
              "0       1      122     5.0  838985046\n",
              "1       1      185     5.0  838983525\n",
              "2       1      231     5.0  838983392\n",
              "3       1      292     5.0  838983421\n",
              "4       1      316     5.0  838983392"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75JhuEB_KbIb"
      },
      "source": [
        "### Data splitting procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdOzp4RfKbIb"
      },
      "source": [
        "- Select 10K users as heldout users, 10K users as validation users, and the rest of the users for training\n",
        "- Use all the items from the training users as item set\n",
        "- For each of both validation and test user, subsample 80% as fold-in data and the rest for prediction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChLy_ymAKbIb"
      },
      "source": [
        "def get_count(tp, id):\n",
        "    playcount_groupbyid = tp[[id]].groupby(id)\n",
        "    count = playcount_groupbyid.size()\n",
        "    return count"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlcITsDZKbIb"
      },
      "source": [
        "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
        "    # Only keep the triplets for items which were clicked on by at least min_sc users. \n",
        "    if min_sc > 0:\n",
        "        itemcount = get_count(tp, 'movieId')\n",
        "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
        "    \n",
        "    # Only keep the triplets for users who clicked on at least min_uc items\n",
        "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
        "    if min_uc > 0:\n",
        "        usercount = get_count(tp, 'userId')\n",
        "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
        "    \n",
        "    # Update both usercount and itemcount after filtering\n",
        "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId') \n",
        "    return tp, usercount, itemcount"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RYQPeH2KbIb"
      },
      "source": [
        "Only keep items that are clicked on by at least 5 users"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQGWw184KbIb"
      },
      "source": [
        "raw_data, user_activity, item_popularity = filter_triplets(raw_data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9-rt2ezKbIb",
        "outputId": "434f2e70-b256-4633-f6d5-f89b7229759b"
      },
      "source": [
        "sparsity = 1. * raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
        "\n",
        "print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" % \n",
        "      (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After filtering, there are 5003786 watching events from 69167 users and 10258 movies (sparsity: 0.705%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbyWcf1pKbIb"
      },
      "source": [
        "unique_uid = user_activity.index\n",
        "\n",
        "np.random.seed(98765)\n",
        "idx_perm = np.random.permutation(unique_uid.size)\n",
        "unique_uid = unique_uid[idx_perm]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTku1H1DKbIb"
      },
      "source": [
        "# create train/validation/test users\n",
        "n_users = unique_uid.size\n",
        "n_heldout_users = 10000\n",
        "\n",
        "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
        "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
        "te_users = unique_uid[(n_users - n_heldout_users):]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdASvG79KbIb"
      },
      "source": [
        "train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne0HiGtOKbIb"
      },
      "source": [
        "unique_sid = pd.unique(train_plays['movieId'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Av46YOzKbIc"
      },
      "source": [
        "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
        "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8MYrWjdKbIc"
      },
      "source": [
        "# pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
        "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
        "\n",
        "if not os.path.exists(pro_dir):\n",
        "    os.makedirs(pro_dir)\n",
        "\n",
        "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
        "    for sid in unique_sid:\n",
        "        f.write('%s\\n' % sid)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW7cBfvNKbIc"
      },
      "source": [
        "def split_train_test_proportion(data, test_prop=0.2):\n",
        "    data_grouped_by_user = data.groupby('userId')\n",
        "    tr_list, te_list = list(), list()\n",
        "\n",
        "    np.random.seed(98765)\n",
        "\n",
        "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
        "        n_items_u = len(group)\n",
        "\n",
        "        if n_items_u >= 5:\n",
        "            idx = np.zeros(n_items_u, dtype='bool')\n",
        "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
        "\n",
        "            tr_list.append(group[np.logical_not(idx)])\n",
        "            te_list.append(group[idx])\n",
        "        else:\n",
        "            tr_list.append(group)\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print(\"%d users sampled\" % i)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    data_tr = pd.concat(tr_list)\n",
        "    data_te = pd.concat(te_list)\n",
        "    \n",
        "    return data_tr, data_te"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLuS5LXwKbIc"
      },
      "source": [
        "vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
        "vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pCxg4OUKbIc",
        "outputId": "3390510d-49bb-4d0b-e3ac-f168a43a5001"
      },
      "source": [
        "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 users sampled\n",
            "1000 users sampled\n",
            "2000 users sampled\n",
            "3000 users sampled\n",
            "4000 users sampled\n",
            "5000 users sampled\n",
            "6000 users sampled\n",
            "7000 users sampled\n",
            "8000 users sampled\n",
            "9000 users sampled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSpz_DX0KbIc"
      },
      "source": [
        "test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
        "test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q_LYlJFKbIc",
        "outputId": "2295b293-e58a-4b5b-d9bb-9c5b0517fe23"
      },
      "source": [
        "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 users sampled\n",
            "1000 users sampled\n",
            "2000 users sampled\n",
            "3000 users sampled\n",
            "4000 users sampled\n",
            "5000 users sampled\n",
            "6000 users sampled\n",
            "7000 users sampled\n",
            "8000 users sampled\n",
            "9000 users sampled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D12r0whSKbIc"
      },
      "source": [
        "### Save the data into (user_index, item_index) format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F_kXEokKbIc"
      },
      "source": [
        "def numerize(tp):\n",
        "    uid = list(map(lambda x: profile2id[x], tp['userId']))\n",
        "    sid = list(map(lambda x: show2id[x], tp['movieId']))\n",
        "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUHPsKhZKbIc"
      },
      "source": [
        "train_data = numerize(train_plays)\n",
        "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRNSTf0QKbIc"
      },
      "source": [
        "vad_data_tr = numerize(vad_plays_tr)\n",
        "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4o0GVKaKbIc"
      },
      "source": [
        "vad_data_te = numerize(vad_plays_te)\n",
        "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KBW_3aqKbIc"
      },
      "source": [
        "test_data_tr = numerize(test_plays_tr)\n",
        "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdB3PRQJKbIc"
      },
      "source": [
        "test_data_te = numerize(test_plays_te)\n",
        "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McZLq8QkKbIc"
      },
      "source": [
        "## Model definition and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIlMAfz4KbId"
      },
      "source": [
        "We define two related models: denoising autoencoder with multinomial likelihood (Multi-DAE in the paper) and partially-regularized variational autoencoder with multinomial likelihood (Multi-VAE^{PR} in the paper)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCPIIYYMKbId"
      },
      "source": [
        "### Model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEcLKHvcKbId"
      },
      "source": [
        "__Notations__: We use $u \\in \\{1,\\dots,U\\}$ to index users and $i \\in \\{1,\\dots,I\\}$ to index items. In this work, we consider learning with implicit feedback. The user-by-item interaction matrix is the click matrix $\\mathbf{X} \\in \\mathbb{N}^{U\\times I}$. The lower case $\\mathbf{x}_u =[X_{u1},\\dots,X_{uI}]^\\top \\in \\mathbb{N}^I$ is a bag-of-words vector with the number of clicks for each item from user u. We binarize the click matrix. It is straightforward to extend it to general count data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvW4hHdrKbId"
      },
      "source": [
        "__Generative process__: For each user $u$, the model starts by sampling a $K$-dimensional latent representation $\\mathbf{z}_u$ from a standard Gaussian prior. The latent representation $\\mathbf{z}_u$ is transformed via a non-linear function $f_\\theta (\\cdot) \\in \\mathbb{R}^I$ to produce a probability distribution over $I$ items $\\pi (\\mathbf{z}_u)$ from which the click history $\\mathbf{x}_u$ is assumed to have been drawn:\n",
        "\n",
        "$$\n",
        "\\mathbf{z}_u \\sim \\mathcal{N}(0, \\mathbf{I}_K),  \\pi(\\mathbf{z}_u) \\propto \\exp\\{f_\\theta (\\mathbf{z}_u\\},\\\\\n",
        "\\mathbf{x}_u \\sim \\mathrm{Mult}(N_u, \\pi(\\mathbf{z}_u))\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dIsvORCKbId"
      },
      "source": [
        "The objective for Multi-DAE for a single user $u$ is:\n",
        "$$\n",
        "\\mathcal{L}_u(\\theta, \\phi) = \\log p_\\theta(\\mathbf{x}_u | g_\\phi(\\mathbf{x}_u))\n",
        "$$\n",
        "where $g_\\phi(\\cdot)$ is the non-linear \"encoder\" function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy2LESs4KbId"
      },
      "source": [
        "class MultiDAE(object):\n",
        "    def __init__(self, p_dims, q_dims=None, lam=0.01, lr=1e-3, random_seed=None):\n",
        "        self.p_dims = p_dims\n",
        "        if q_dims is None:\n",
        "            self.q_dims = p_dims[::-1]\n",
        "        else:\n",
        "            assert q_dims[0] == p_dims[-1], \"Input and output dimension must equal each other for autoencoders.\"\n",
        "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q-network mismatches.\"\n",
        "            self.q_dims = q_dims\n",
        "        self.dims = self.q_dims + self.p_dims[1:]\n",
        "        \n",
        "        self.lam = lam\n",
        "        self.lr = lr\n",
        "        self.random_seed = random_seed\n",
        "\n",
        "        self.construct_placeholders()\n",
        "\n",
        "    def construct_placeholders(self):        \n",
        "        self.input_ph = tf.placeholder(\n",
        "            dtype=tf.float32, shape=[None, self.dims[0]])\n",
        "        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=None)\n",
        "\n",
        "    def build_graph(self):\n",
        "\n",
        "        self.construct_weights()\n",
        "\n",
        "        saver, logits = self.forward_pass()\n",
        "        log_softmax_var = tf.nn.log_softmax(logits)\n",
        "\n",
        "        # per-user average negative log-likelihood\n",
        "        neg_ll = -tf.reduce_mean(tf.reduce_sum(\n",
        "            log_softmax_var * self.input_ph, axis=1))\n",
        "        # apply regularization to weights\n",
        "        reg = l2_regularizer(self.lam)\n",
        "        reg_var = apply_regularization(reg, self.weights)\n",
        "        # tensorflow l2 regularization multiply 0.5 to the l2 norm\n",
        "        # multiply 2 so that it is back in the same scale\n",
        "        loss = neg_ll + 2 * reg_var\n",
        "        \n",
        "        train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\n",
        "\n",
        "        # add summary statistics\n",
        "        tf.summary.scalar('negative_multi_ll', neg_ll)\n",
        "        tf.summary.scalar('loss', loss)\n",
        "        merged = tf.summary.merge_all()\n",
        "        return saver, logits, loss, train_op, merged\n",
        "\n",
        "    def forward_pass(self):\n",
        "        # construct forward graph        \n",
        "        h = tf.nn.l2_normalize(self.input_ph, 1)\n",
        "        h = tf.nn.dropout(h, self.keep_prob_ph)\n",
        "        \n",
        "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
        "            h = tf.matmul(h, w) + b\n",
        "            \n",
        "            if i != len(self.weights) - 1:\n",
        "                h = tf.nn.tanh(h)\n",
        "        return tf.train.Saver(), h\n",
        "\n",
        "    def construct_weights(self):\n",
        "\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        \n",
        "        # define weights\n",
        "        for i, (d_in, d_out) in enumerate(zip(self.dims[:-1], self.dims[1:])):\n",
        "            weight_key = \"weight_{}to{}\".format(i, i+1)\n",
        "            bias_key = \"bias_{}\".format(i+1)\n",
        "            \n",
        "            self.weights.append(tf.get_variable(\n",
        "                name=weight_key, shape=[d_in, d_out],\n",
        "                initializer=tf.contrib.layers.xavier_initializer(\n",
        "                    seed=self.random_seed)))\n",
        "            \n",
        "            self.biases.append(tf.get_variable(\n",
        "                name=bias_key, shape=[d_out],\n",
        "                initializer=tf.truncated_normal_initializer(\n",
        "                    stddev=0.001, seed=self.random_seed)))\n",
        "            \n",
        "            # add summary stats\n",
        "            tf.summary.histogram(weight_key, self.weights[-1])\n",
        "            tf.summary.histogram(bias_key, self.biases[-1])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P1xXakVKbId"
      },
      "source": [
        "The objective of Multi-VAE^{PR} (evidence lower-bound, or ELBO) for a single user $u$ is:\n",
        "$$\n",
        "\\mathcal{L}_u(\\theta, \\phi) = \\mathbb{E}_{q_\\phi(z_u | x_u)}[\\log p_\\theta(x_u | z_u)] - \\beta \\cdot KL(q_\\phi(z_u | x_u) \\| p(z_u))\n",
        "$$\n",
        "where $q_\\phi$ is the approximating variational distribution (inference model). $\\beta$ is the additional annealing parameter that we control. The objective of the entire dataset is the average over all the users. It can be trained almost the same as Multi-DAE, thanks to reparametrization trick. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94iVFyI_KbId"
      },
      "source": [
        "class MultiVAE(MultiDAE):\n",
        "\n",
        "    def construct_placeholders(self):\n",
        "        super(MultiVAE, self).construct_placeholders()\n",
        "\n",
        "        # placeholders with default values when scoring\n",
        "        self.is_training_ph = tf.placeholder_with_default(0., shape=None)\n",
        "        self.anneal_ph = tf.placeholder_with_default(1., shape=None)\n",
        "        \n",
        "    def build_graph(self):\n",
        "        self._construct_weights()\n",
        "\n",
        "        saver, logits, KL = self.forward_pass()\n",
        "        log_softmax_var = tf.nn.log_softmax(logits)\n",
        "\n",
        "        neg_ll = -tf.reduce_mean(tf.reduce_sum(\n",
        "            log_softmax_var * self.input_ph,\n",
        "            axis=-1))\n",
        "        # apply regularization to weights\n",
        "        reg = l2_regularizer(self.lam)\n",
        "        reg_var = apply_regularization(reg, self.weights_q + self.weights_p)\n",
        "        # tensorflow l2 regularization multiply 0.5 to the l2 norm\n",
        "        # multiply 2 so that it is back in the same scale\n",
        "        neg_ELBO = neg_ll + self.anneal_ph * KL + 2 * reg_var\n",
        "        \n",
        "        train_op = tf.train.AdamOptimizer(self.lr).minimize(neg_ELBO)\n",
        "\n",
        "        # add summary statistics\n",
        "        tf.summary.scalar('negative_multi_ll', neg_ll)\n",
        "        tf.summary.scalar('KL', KL)\n",
        "        tf.summary.scalar('neg_ELBO_train', neg_ELBO)\n",
        "        merged = tf.summary.merge_all()\n",
        "\n",
        "        return saver, logits, neg_ELBO, train_op, merged\n",
        "    \n",
        "    def q_graph(self):\n",
        "        mu_q, std_q, KL = None, None, None\n",
        "        \n",
        "        h = tf.nn.l2_normalize(self.input_ph, 1)\n",
        "        h = tf.nn.dropout(h, self.keep_prob_ph)\n",
        "        \n",
        "        for i, (w, b) in enumerate(zip(self.weights_q, self.biases_q)):\n",
        "            h = tf.matmul(h, w) + b\n",
        "            \n",
        "            if i != len(self.weights_q) - 1:\n",
        "                h = tf.nn.tanh(h)\n",
        "            else:\n",
        "                mu_q = h[:, :self.q_dims[-1]]\n",
        "                logvar_q = h[:, self.q_dims[-1]:]\n",
        "\n",
        "                std_q = tf.exp(0.5 * logvar_q)\n",
        "                KL = tf.reduce_mean(tf.reduce_sum(\n",
        "                        0.5 * (-logvar_q + tf.exp(logvar_q) + mu_q**2 - 1), axis=1))\n",
        "        return mu_q, std_q, KL\n",
        "\n",
        "    def p_graph(self, z):\n",
        "        h = z\n",
        "        \n",
        "        for i, (w, b) in enumerate(zip(self.weights_p, self.biases_p)):\n",
        "            h = tf.matmul(h, w) + b\n",
        "            \n",
        "            if i != len(self.weights_p) - 1:\n",
        "                h = tf.nn.tanh(h)\n",
        "        return h\n",
        "\n",
        "    def forward_pass(self):\n",
        "        # q-network\n",
        "        mu_q, std_q, KL = self.q_graph()\n",
        "        epsilon = tf.random_normal(tf.shape(std_q))\n",
        "\n",
        "        sampled_z = mu_q + self.is_training_ph *\\\n",
        "            epsilon * std_q\n",
        "\n",
        "        # p-network\n",
        "        logits = self.p_graph(sampled_z)\n",
        "        \n",
        "        return tf.train.Saver(), logits, KL\n",
        "\n",
        "    def _construct_weights(self):\n",
        "        self.weights_q, self.biases_q = [], []\n",
        "        \n",
        "        for i, (d_in, d_out) in enumerate(zip(self.q_dims[:-1], self.q_dims[1:])):\n",
        "            if i == len(self.q_dims[:-1]) - 1:\n",
        "                # we need two sets of parameters for mean and variance,\n",
        "                # respectively\n",
        "                d_out *= 2\n",
        "            weight_key = \"weight_q_{}to{}\".format(i, i+1)\n",
        "            bias_key = \"bias_q_{}\".format(i+1)\n",
        "            \n",
        "            self.weights_q.append(tf.get_variable(\n",
        "                name=weight_key, shape=[d_in, d_out],\n",
        "                initializer=tf.contrib.layers.xavier_initializer(\n",
        "                    seed=self.random_seed)))\n",
        "            \n",
        "            self.biases_q.append(tf.get_variable(\n",
        "                name=bias_key, shape=[d_out],\n",
        "                initializer=tf.truncated_normal_initializer(\n",
        "                    stddev=0.001, seed=self.random_seed)))\n",
        "            \n",
        "            # add summary stats\n",
        "            tf.summary.histogram(weight_key, self.weights_q[-1])\n",
        "            tf.summary.histogram(bias_key, self.biases_q[-1])\n",
        "            \n",
        "        self.weights_p, self.biases_p = [], []\n",
        "\n",
        "        for i, (d_in, d_out) in enumerate(zip(self.p_dims[:-1], self.p_dims[1:])):\n",
        "            weight_key = \"weight_p_{}to{}\".format(i, i+1)\n",
        "            bias_key = \"bias_p_{}\".format(i+1)\n",
        "            self.weights_p.append(tf.get_variable(\n",
        "                name=weight_key, shape=[d_in, d_out],\n",
        "                initializer=tf.contrib.layers.xavier_initializer(\n",
        "                    seed=self.random_seed)))\n",
        "            \n",
        "            self.biases_p.append(tf.get_variable(\n",
        "                name=bias_key, shape=[d_out],\n",
        "                initializer=tf.truncated_normal_initializer(\n",
        "                    stddev=0.001, seed=self.random_seed)))\n",
        "            \n",
        "            # add summary stats\n",
        "            tf.summary.histogram(weight_key, self.weights_p[-1])\n",
        "            tf.summary.histogram(bias_key, self.biases_p[-1])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhsnuTDKbId"
      },
      "source": [
        "### Training/validation data, hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHeuM78SKbId"
      },
      "source": [
        "Load the pre-processed training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzi4hI4uKbId"
      },
      "source": [
        "unique_sid = list()\n",
        "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'r') as f:\n",
        "    for line in f:\n",
        "        unique_sid.append(line.strip())\n",
        "\n",
        "n_items = len(unique_sid)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nrgFBSQKbId"
      },
      "source": [
        "def load_train_data(csv_file):\n",
        "    tp = pd.read_csv(csv_file)\n",
        "    n_users = tp['uid'].max() + 1\n",
        "\n",
        "    rows, cols = tp['uid'], tp['sid']\n",
        "    data = sparse.csr_matrix((np.ones_like(rows),\n",
        "                             (rows, cols)), dtype='float64',\n",
        "                             shape=(n_users, n_items))\n",
        "    return data"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVN2K0N5KbId"
      },
      "source": [
        "train_data = load_train_data(os.path.join(pro_dir, 'train.csv'))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1zbIgdXKbId"
      },
      "source": [
        "def load_tr_te_data(csv_file_tr, csv_file_te):\n",
        "    tp_tr = pd.read_csv(csv_file_tr)\n",
        "    tp_te = pd.read_csv(csv_file_te)\n",
        "\n",
        "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
        "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
        "\n",
        "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
        "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
        "\n",
        "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
        "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
        "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
        "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
        "    return data_tr, data_te"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWW9jkv8KbId"
      },
      "source": [
        "vad_data_tr, vad_data_te = load_tr_te_data(os.path.join(pro_dir, 'validation_tr.csv'),\n",
        "                                           os.path.join(pro_dir, 'validation_te.csv'))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ2uu1M_KbId"
      },
      "source": [
        "Set up training hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epbUOpjQKbId"
      },
      "source": [
        "N = train_data.shape[0]\n",
        "idxlist = list(range(N))\n",
        "\n",
        "# training batch size\n",
        "batch_size = 500\n",
        "batches_per_epoch = int(np.ceil(float(N) / batch_size))\n",
        "\n",
        "N_vad = vad_data_tr.shape[0]\n",
        "idxlist_vad = list(range(N_vad))\n",
        "\n",
        "# validation batch size (since the entire validation set might not fit into GPU memory)\n",
        "batch_size_vad = 2000\n",
        "\n",
        "# the total number of gradient updates for annealing\n",
        "total_anneal_steps = 200000\n",
        "# largest annealing parameter\n",
        "anneal_cap = 0.2"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe8WpfC4KbId"
      },
      "source": [
        "Evaluate function: Normalized discounted cumulative gain (NDCG@k) and Recall@k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpg3DDfBKbId"
      },
      "source": [
        "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
        "    '''\n",
        "    normalized discounted cumulative gain@k for binary relevance\n",
        "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
        "    '''\n",
        "    batch_users = X_pred.shape[0]\n",
        "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
        "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
        "                       idx_topk_part[:, :k]]\n",
        "    idx_part = np.argsort(-topk_part, axis=1)\n",
        "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
        "    # topk predicted score\n",
        "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
        "    # build the discount template\n",
        "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
        "\n",
        "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
        "                         idx_topk].toarray() * tp).sum(axis=1)\n",
        "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
        "                     for n in heldout_batch.getnnz(axis=1)])\n",
        "    return DCG / IDCG"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvMW1xduKbId"
      },
      "source": [
        "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
        "    batch_users = X_pred.shape[0]\n",
        "\n",
        "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
        "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
        "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
        "\n",
        "    X_true_binary = (heldout_batch > 0).toarray()\n",
        "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
        "        np.float32)\n",
        "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
        "    return recall"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KklHu46iKbId"
      },
      "source": [
        "### Train a Multi-VAE^{PR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIxPZcRwKbId"
      },
      "source": [
        "For ML-20M dataset, we set both the generative function $f_\\theta(\\cdot)$ and the inference model $g_\\phi(\\cdot)$ to be 3-layer multilayer perceptron (MLP) with symmetrical architecture. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txnn1fc1KbId"
      },
      "source": [
        "The generative function is a [200 -> 600 -> n_items] MLP, which means the inference function is a [n_items -> 600 -> 200] MLP. Thus the overall architecture for the Multi-VAE^{PR} is [n_items -> 600 -> 200 -> 600 -> n_items]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zguWFBo8KbIe"
      },
      "source": [
        "p_dims = [200, 600, n_items]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJkeTnt4KbIe",
        "outputId": "dd3345fd-8241-4817-f896-018c1f9debff"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "vae = MultiVAE(p_dims, lam=0.0, random_seed=98765)\n",
        "\n",
        "saver, logits_var, loss_var, train_op_var, merged_var = vae.build_graph()\n",
        "\n",
        "ndcg_var = tf.Variable(0.0)\n",
        "ndcg_dist_var = tf.placeholder(dtype=tf.float64, shape=None)\n",
        "ndcg_summary = tf.summary.scalar('ndcg_at_k_validation', ndcg_var)\n",
        "ndcg_dist_summary = tf.summary.histogram('ndcg_at_k_hist_validation', ndcg_dist_var)\n",
        "merged_valid = tf.summary.merge([ndcg_summary, ndcg_dist_summary])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-29-238c8eded668>:40: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5U-tOm1KbIe"
      },
      "source": [
        "Set up logging and checkpoint directory\n",
        "\n",
        "- Change all the logging directory and checkpoint directory to somewhere of your choice\n",
        "- Monitor training progress using tensorflow by: `tensorboard --logdir=$log_dir`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYqzMPB0KbIe"
      },
      "source": [
        "arch_str = \"I-%s-I\" % ('-'.join([str(d) for d in vae.dims[1:-1]]))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_6LvF7JKbIe",
        "outputId": "0aa63432-f8a6-42e1-e594-3503516ccc39"
      },
      "source": [
        "log_dir = '/content/drive/My Drive/Study/Project3/ml-10m/log/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
        "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
        "\n",
        "if os.path.exists(log_dir):\n",
        "    shutil.rmtree(log_dir)\n",
        "\n",
        "print(\"log directory: %s\" % log_dir)\n",
        "summary_writer = tf.summary.FileWriter(log_dir, graph=tf.get_default_graph())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log directory: /content/drive/My Drive/Study/Project3/ml-10m/log/VAE_anneal200.0K_cap2.0E-01/I-600-200-600-I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtRExyrSKbIe",
        "outputId": "f5621bb9-83b0-4274-bd9f-79febc3453ff"
      },
      "source": [
        "chkpt_dir = '/content/drive/My Drive/Study/Project3/ml-10m/checkpoints/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
        "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
        "\n",
        "if not os.path.isdir(chkpt_dir):\n",
        "    os.makedirs(chkpt_dir) \n",
        "    \n",
        "print(\"chkpt directory: %s\" % chkpt_dir)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chkpt directory: /content/drive/My Drive/Study/Project3/ml-10m/checkpoints/VAE_anneal200.0K_cap2.0E-01/I-600-200-600-I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSJJOgKJKbIf"
      },
      "source": [
        "n_epochs = 150"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGnvfwA-KbIf"
      },
      "source": [
        "ndcgs_vad = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "\n",
        "    best_ndcg = -np.inf\n",
        "\n",
        "    update_count = 0.0\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        np.random.shuffle(idxlist)\n",
        "        # train for one epoch\n",
        "        for bnum, st_idx in enumerate(range(0, N, batch_size)):\n",
        "            end_idx = min(st_idx + batch_size, N)\n",
        "            X = train_data[idxlist[st_idx:end_idx]]\n",
        "            \n",
        "            if sparse.isspmatrix(X):\n",
        "                X = X.toarray()\n",
        "            X = X.astype('float32')           \n",
        "            \n",
        "            if total_anneal_steps > 0:\n",
        "                anneal = min(anneal_cap, 1. * update_count / total_anneal_steps)\n",
        "            else:\n",
        "                anneal = anneal_cap\n",
        "            \n",
        "            feed_dict = {vae.input_ph: X, \n",
        "                         vae.keep_prob_ph: 0.5, \n",
        "                         vae.anneal_ph: anneal,\n",
        "                         vae.is_training_ph: 1}        \n",
        "            sess.run(train_op_var, feed_dict=feed_dict)\n",
        "\n",
        "            if bnum % 100 == 0:\n",
        "                summary_train = sess.run(merged_var, feed_dict=feed_dict)\n",
        "                summary_writer.add_summary(summary_train, \n",
        "                                           global_step=epoch * batches_per_epoch + bnum) \n",
        "            \n",
        "            update_count += 1\n",
        "        \n",
        "        # compute validation NDCG\n",
        "        ndcg_dist = []\n",
        "        for bnum, st_idx in enumerate(range(0, N_vad, batch_size_vad)):\n",
        "            end_idx = min(st_idx + batch_size_vad, N_vad)\n",
        "            X = vad_data_tr[idxlist_vad[st_idx:end_idx]]\n",
        "\n",
        "            if sparse.isspmatrix(X):\n",
        "                X = X.toarray()\n",
        "            X = X.astype('float32')\n",
        "        \n",
        "            pred_val = sess.run(logits_var, feed_dict={vae.input_ph: X} )\n",
        "            # exclude examples from training and validation (if any)\n",
        "            pred_val[X.nonzero()] = -np.inf\n",
        "            ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, vad_data_te[idxlist_vad[st_idx:end_idx]]))\n",
        "        \n",
        "        ndcg_dist = np.concatenate(ndcg_dist)\n",
        "        ndcg_ = ndcg_dist.mean()\n",
        "        ndcgs_vad.append(ndcg_)\n",
        "        merged_valid_val = sess.run(merged_valid, feed_dict={ndcg_var: ndcg_, ndcg_dist_var: ndcg_dist})\n",
        "        summary_writer.add_summary(merged_valid_val, epoch)\n",
        "\n",
        "        # update the best model (if necessary)\n",
        "        if ndcg_ > best_ndcg:\n",
        "            saver.save(sess, '{}/model'.format(chkpt_dir))\n",
        "            best_ndcg = ndcg_"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YxiD4pwKbIf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "7dd8e8e8-ab74-40d1-833a-8fa97231d0df"
      },
      "source": [
        "plt.figure(figsize=(12, 3))\n",
        "plt.plot(ndcgs_vad)\n",
        "plt.ylabel(\"Validation NDCG@100\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "pass"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMdqcRMWKbIf"
      },
      "source": [
        "### Load the test data and compute test metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wOALoUOKbIf"
      },
      "source": [
        "test_data_tr, test_data_te = load_tr_te_data(\n",
        "    os.path.join(pro_dir, 'test_tr.csv'),\n",
        "    os.path.join(pro_dir, 'test_te.csv'))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bJdNtXpKbIf"
      },
      "source": [
        "N_test = test_data_tr.shape[0]\n",
        "idxlist_test = range(N_test)\n",
        "\n",
        "batch_size_test = 2000"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z14AeVaKbIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdeffe1c-d843-4e61-9c2f-d01882e770dd"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "vae = MultiVAE(p_dims, lam=0.0)\n",
        "saver, logits_var, _, _, _ = vae.build_graph()    "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Scale of 0 disables regularizer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IfeVohUKbIf"
      },
      "source": [
        "Load the best performing model on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axVRJgY-KbIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e67648ce-5d8c-4b63-acb6-4082bbfeb497"
      },
      "source": [
        "chkpt_dir = '/content/drive/My Drive/Study/Project3/ml-10m/checkpoints/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
        "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
        "print(\"chkpt directory: %s\" % chkpt_dir)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chkpt directory: /content/drive/My Drive/Study/Project3/ml-10m/checkpoints/VAE_anneal200.0K_cap2.0E-01/I-600-200-600-I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR3wWxPvKbIg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f73149-5ecc-46e4-bf76-3845589475a1"
      },
      "source": [
        "n100_list, r20_list, r50_list = [], [], []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, '{}/model'.format(chkpt_dir))\n",
        "\n",
        "    for bnum, st_idx in enumerate(range(0, N_test, batch_size_test)):\n",
        "        end_idx = min(st_idx + batch_size_test, N_test)\n",
        "        X = test_data_tr[idxlist_test[st_idx:end_idx]]\n",
        "\n",
        "        if sparse.isspmatrix(X):\n",
        "            X = X.toarray()\n",
        "        X = X.astype('float32')\n",
        "\n",
        "        pred_val = sess.run(logits_var, feed_dict={vae.input_ph: X})\n",
        "        # exclude examples from training and validation (if any)\n",
        "        pred_val[X.nonzero()] = -np.inf\n",
        "        n100_list.append(NDCG_binary_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=100))\n",
        "        r20_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=20))\n",
        "        r50_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=50))\n",
        "    \n",
        "n100_list = np.concatenate(n100_list)\n",
        "r20_list = np.concatenate(r20_list)\n",
        "r50_list = np.concatenate(r50_list)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Study/Project3/ml-10m/checkpoints/VAE_anneal200.0K_cap2.0E-01/I-600-200-600-I/model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQW1bzwkKbIg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bbca224-9273-4242-d9c1-33bfa9f86b10"
      },
      "source": [
        "print(\"Test NDCG@100=%.5f (%.5f)\" % (np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))))\n",
        "print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
        "print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list), np.std(r50_list) / np.sqrt(len(r50_list))))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test NDCG@100=0.43144 (0.00208)\n",
            "Test Recall@20=0.40324 (0.00275)\n",
            "Test Recall@50=0.54982 (0.00291)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}