{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "vae_10m_.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhQ8g-UDLpqn",
        "outputId": "18fc2c89-4035-4eba-b464-5f857454da35"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y-MU2H5KbIZ"
      },
      "source": [
        "# Variational autoencoders for collaborative filtering "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBvM4FolKbIZ"
      },
      "source": [
        "This notebook accompanies the paper \"*Variational autoencoders for collaborative filtering*\" by Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara, in The Web Conference (aka WWW) 2018.\n",
        "\n",
        "In this notebook, we will show a complete self-contained example of training a variational autoencoder (as well as a denoising autoencoder) with multinomial likelihood (described in the paper) on the public Movielens-20M dataset, including both data preprocessing and model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqHVSXvVKbIa"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sn\n",
        "sn.set()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.layers import apply_regularization, l2_regularizer\n",
        "import bottleneck as bn\n",
        "from google.colab import drive"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jSKlwM2KbIa"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FfmcWszKbIa"
      },
      "source": [
        "We load the data and create train/validation/test splits following strong generalization: \n",
        "\n",
        "- We split all users into training/validation/test sets. \n",
        "\n",
        "- We train models using the entire click history of the training users. \n",
        "\n",
        "- To evaluate, we take part of the click history from held-out (validation and test) users to learn the necessary user-level representations for the model and then compute metrics by looking at how well the model ranks the rest of the unseen click history from the held-out users."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6aQkgc3KbIa",
        "outputId": "bb3aa7e0-8624-47fd-abc3-63ec973a3b01"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "### change `DATA_DIR` to the location where movielens-20m dataset sits\n",
        "DATA_DIR = '/content/drive/My Drive/Study/Project3/ml-10m'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qBQWbvSKbIa"
      },
      "source": [
        "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'ratings.csv'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq7xOWy9KbIa"
      },
      "source": [
        "# binarize the data (only keep ratings >= 4)\n",
        "raw_data = raw_data[raw_data['rating'] > 3.5]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "C_YwUsZwKbIa",
        "outputId": "969a5474-2c49-4a6f-ac57-3af4a686f001"
      },
      "source": [
        "raw_data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>122</td>\n",
              "      <td>5.0</td>\n",
              "      <td>838985046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>185</td>\n",
              "      <td>5.0</td>\n",
              "      <td>838983525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>231</td>\n",
              "      <td>5.0</td>\n",
              "      <td>838983392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>292</td>\n",
              "      <td>5.0</td>\n",
              "      <td>838983421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>316</td>\n",
              "      <td>5.0</td>\n",
              "      <td>838983392</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   userId  movieId  rating  timestamp\n",
              "0       1      122     5.0  838985046\n",
              "1       1      185     5.0  838983525\n",
              "2       1      231     5.0  838983392\n",
              "3       1      292     5.0  838983421\n",
              "4       1      316     5.0  838983392"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75JhuEB_KbIb"
      },
      "source": [
        "### Data splitting procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdOzp4RfKbIb"
      },
      "source": [
        "- Select 10% of total users as heldout users, 10% of total users as validation users, and the rest of the users for training\n",
        "- Use all the items from the training users as item set\n",
        "- For each of both validation and test user, subsample 80% as fold-in data and the rest for prediction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChLy_ymAKbIb"
      },
      "source": [
        "def get_count(tp, id):\n",
        "    playcount_groupbyid = tp[[id]].groupby(id)\n",
        "    count = playcount_groupbyid.size()\n",
        "    return count"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlcITsDZKbIb"
      },
      "source": [
        "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
        "    # Only keep the triplets for items which were clicked on by at least min_sc users. \n",
        "    if min_sc > 0:\n",
        "        itemcount = get_count(tp, 'movieId')\n",
        "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
        "    \n",
        "    # Only keep the triplets for users who clicked on at least min_uc items\n",
        "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
        "    if min_uc > 0:\n",
        "        usercount = get_count(tp, 'userId')\n",
        "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
        "    \n",
        "    # Update both usercount and itemcount after filtering\n",
        "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId') \n",
        "    return tp, usercount, itemcount"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RYQPeH2KbIb"
      },
      "source": [
        "Only keep items that are clicked on by at least 5 users"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQGWw184KbIb"
      },
      "source": [
        "raw_data, user_activity, item_popularity = filter_triplets(raw_data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9-rt2ezKbIb",
        "outputId": "824db9cb-1dfa-44a4-f679-d2a82a8c1c94"
      },
      "source": [
        "sparsity = 1. * raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
        "\n",
        "print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" % \n",
        "      (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After filtering, there are 5003786 watching events from 69167 users and 10258 movies (sparsity: 0.705%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbyWcf1pKbIb"
      },
      "source": [
        "unique_uid = user_activity.index\n",
        "\n",
        "np.random.seed(98765)\n",
        "idx_perm = np.random.permutation(unique_uid.size)\n",
        "unique_uid = unique_uid[idx_perm]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTku1H1DKbIb"
      },
      "source": [
        "# create train/validation/test users\n",
        "n_users = unique_uid.size\n",
        "n_heldout_users = int(n_users/10)\n",
        "\n",
        "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
        "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
        "te_users = unique_uid[(n_users - n_heldout_users):]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdASvG79KbIb"
      },
      "source": [
        "train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne0HiGtOKbIb"
      },
      "source": [
        "unique_sid = pd.unique(train_plays['movieId'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Av46YOzKbIc"
      },
      "source": [
        "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
        "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8MYrWjdKbIc"
      },
      "source": [
        "# pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
        "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
        "\n",
        "if not os.path.exists(pro_dir):\n",
        "    os.makedirs(pro_dir)\n",
        "\n",
        "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
        "    for sid in unique_sid:\n",
        "        f.write('%s\\n' % sid)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW7cBfvNKbIc"
      },
      "source": [
        "def split_train_test_proportion(data, test_prop=0.2):\n",
        "    data_grouped_by_user = data.groupby('userId')\n",
        "    tr_list, te_list = list(), list()\n",
        "\n",
        "    np.random.seed(98765)\n",
        "\n",
        "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
        "        n_items_u = len(group)\n",
        "\n",
        "        if n_items_u >= 5:\n",
        "            idx = np.zeros(n_items_u, dtype='bool')\n",
        "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
        "\n",
        "            tr_list.append(group[np.logical_not(idx)])\n",
        "            te_list.append(group[idx])\n",
        "        else:\n",
        "            tr_list.append(group)\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print(\"%d users sampled\" % i)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    data_tr = pd.concat(tr_list)\n",
        "    data_te = pd.concat(te_list)\n",
        "    \n",
        "    return data_tr, data_te"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLuS5LXwKbIc"
      },
      "source": [
        "vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
        "vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pCxg4OUKbIc",
        "outputId": "92a74bba-6144-4489-9b32-c031ee265dca"
      },
      "source": [
        "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 users sampled\n",
            "1000 users sampled\n",
            "2000 users sampled\n",
            "3000 users sampled\n",
            "4000 users sampled\n",
            "5000 users sampled\n",
            "6000 users sampled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSpz_DX0KbIc"
      },
      "source": [
        "test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
        "test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q_LYlJFKbIc",
        "outputId": "7fce4160-b4e4-4c2a-8ea7-63cb6c8c3f57"
      },
      "source": [
        "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 users sampled\n",
            "1000 users sampled\n",
            "2000 users sampled\n",
            "3000 users sampled\n",
            "4000 users sampled\n",
            "5000 users sampled\n",
            "6000 users sampled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D12r0whSKbIc"
      },
      "source": [
        "### Save the data into (user_index, item_index) format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F_kXEokKbIc"
      },
      "source": [
        "def numerize(tp):\n",
        "    uid = list(map(lambda x: profile2id[x], tp['userId']))\n",
        "    sid = list(map(lambda x: show2id[x], tp['movieId']))\n",
        "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUHPsKhZKbIc"
      },
      "source": [
        "train_data = numerize(train_plays)\n",
        "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRNSTf0QKbIc"
      },
      "source": [
        "vad_data_tr = numerize(vad_plays_tr)\n",
        "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4o0GVKaKbIc"
      },
      "source": [
        "vad_data_te = numerize(vad_plays_te)\n",
        "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KBW_3aqKbIc"
      },
      "source": [
        "test_data_tr = numerize(test_plays_tr)\n",
        "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdB3PRQJKbIc"
      },
      "source": [
        "test_data_te = numerize(test_plays_te)\n",
        "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McZLq8QkKbIc"
      },
      "source": [
        "## Model definition and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIlMAfz4KbId"
      },
      "source": [
        "We define two related models: denoising autoencoder with multinomial likelihood (Multi-DAE in the paper) and partially-regularized variational autoencoder with multinomial likelihood (Multi-VAE^{PR} in the paper)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCPIIYYMKbId"
      },
      "source": [
        "### Model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEcLKHvcKbId"
      },
      "source": [
        "__Notations__: We use $u \\in \\{1,\\dots,U\\}$ to index users and $i \\in \\{1,\\dots,I\\}$ to index items. In this work, we consider learning with implicit feedback. The user-by-item interaction matrix is the click matrix $\\mathbf{X} \\in \\mathbb{N}^{U\\times I}$. The lower case $\\mathbf{x}_u =[X_{u1},\\dots,X_{uI}]^\\top \\in \\mathbb{N}^I$ is a bag-of-words vector with the number of clicks for each item from user u. We binarize the click matrix. It is straightforward to extend it to general count data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvW4hHdrKbId"
      },
      "source": [
        "__Generative process__: For each user $u$, the model starts by sampling a $K$-dimensional latent representation $\\mathbf{z}_u$ from a standard Gaussian prior. The latent representation $\\mathbf{z}_u$ is transformed via a non-linear function $f_\\theta (\\cdot) \\in \\mathbb{R}^I$ to produce a probability distribution over $I$ items $\\pi (\\mathbf{z}_u)$ from which the click history $\\mathbf{x}_u$ is assumed to have been drawn:\n",
        "\n",
        "$$\n",
        "\\mathbf{z}_u \\sim \\mathcal{N}(0, \\mathbf{I}_K),  \\pi(\\mathbf{z}_u) \\propto \\exp\\{f_\\theta (\\mathbf{z}_u\\},\\\\\n",
        "\\mathbf{x}_u \\sim \\mathrm{Mult}(N_u, \\pi(\\mathbf{z}_u))\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dIsvORCKbId"
      },
      "source": [
        "The objective for Multi-DAE for a single user $u$ is:\n",
        "$$\n",
        "\\mathcal{L}_u(\\theta, \\phi) = \\log p_\\theta(\\mathbf{x}_u | g_\\phi(\\mathbf{x}_u))\n",
        "$$\n",
        "where $g_\\phi(\\cdot)$ is the non-linear \"encoder\" function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy2LESs4KbId"
      },
      "source": [
        "class MultiDAE(object):\n",
        "    def __init__(self, p_dims, q_dims=None, lam=0.01, lr=1e-3, random_seed=None):\n",
        "        self.p_dims = p_dims\n",
        "        if q_dims is None:\n",
        "            self.q_dims = p_dims[::-1]\n",
        "        else:\n",
        "            assert q_dims[0] == p_dims[-1], \"Input and output dimension must equal each other for autoencoders.\"\n",
        "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q-network mismatches.\"\n",
        "            self.q_dims = q_dims\n",
        "        self.dims = self.q_dims + self.p_dims[1:]\n",
        "        \n",
        "        self.lam = lam\n",
        "        self.lr = lr\n",
        "        self.random_seed = random_seed\n",
        "\n",
        "        self.construct_placeholders()\n",
        "\n",
        "    def construct_placeholders(self):        \n",
        "        self.input_ph = tf.placeholder(\n",
        "            dtype=tf.float32, shape=[None, self.dims[0]])\n",
        "        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=None)\n",
        "\n",
        "    def build_graph(self):\n",
        "\n",
        "        self.construct_weights()\n",
        "\n",
        "        saver, logits = self.forward_pass()\n",
        "        log_softmax_var = tf.nn.log_softmax(logits)\n",
        "\n",
        "        # per-user average negative log-likelihood\n",
        "        neg_ll = -tf.reduce_mean(tf.reduce_sum(\n",
        "            log_softmax_var * self.input_ph, axis=1))\n",
        "        # apply regularization to weights\n",
        "        reg = l2_regularizer(self.lam)\n",
        "        reg_var = apply_regularization(reg, self.weights)\n",
        "        # tensorflow l2 regularization multiply 0.5 to the l2 norm\n",
        "        # multiply 2 so that it is back in the same scale\n",
        "        loss = neg_ll + 2 * reg_var\n",
        "        \n",
        "        train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\n",
        "\n",
        "        # add summary statistics\n",
        "        tf.summary.scalar('negative_multi_ll', neg_ll)\n",
        "        tf.summary.scalar('loss', loss)\n",
        "        merged = tf.summary.merge_all()\n",
        "        return saver, logits, loss, train_op, merged\n",
        "\n",
        "    def forward_pass(self):\n",
        "        # construct forward graph        \n",
        "        h = tf.nn.l2_normalize(self.input_ph, 1)\n",
        "        h = tf.nn.dropout(h, self.keep_prob_ph)\n",
        "        \n",
        "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
        "            h = tf.matmul(h, w) + b\n",
        "            \n",
        "            if i != len(self.weights) - 1:\n",
        "                h = tf.nn.tanh(h)\n",
        "        return tf.train.Saver(), h\n",
        "\n",
        "    def construct_weights(self):\n",
        "\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        \n",
        "        # define weights\n",
        "        for i, (d_in, d_out) in enumerate(zip(self.dims[:-1], self.dims[1:])):\n",
        "            weight_key = \"weight_{}to{}\".format(i, i+1)\n",
        "            bias_key = \"bias_{}\".format(i+1)\n",
        "            \n",
        "            self.weights.append(tf.get_variable(\n",
        "                name=weight_key, shape=[d_in, d_out],\n",
        "                initializer=tf.contrib.layers.xavier_initializer(\n",
        "                    seed=self.random_seed)))\n",
        "            \n",
        "            self.biases.append(tf.get_variable(\n",
        "                name=bias_key, shape=[d_out],\n",
        "                initializer=tf.truncated_normal_initializer(\n",
        "                    stddev=0.001, seed=self.random_seed)))\n",
        "            \n",
        "            # add summary stats\n",
        "            tf.summary.histogram(weight_key, self.weights[-1])\n",
        "            tf.summary.histogram(bias_key, self.biases[-1])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P1xXakVKbId"
      },
      "source": [
        "The objective of Multi-VAE^{PR} (evidence lower-bound, or ELBO) for a single user $u$ is:\n",
        "$$\n",
        "\\mathcal{L}_u(\\theta, \\phi) = \\mathbb{E}_{q_\\phi(z_u | x_u)}[\\log p_\\theta(x_u | z_u)] - \\beta \\cdot KL(q_\\phi(z_u | x_u) \\| p(z_u))\n",
        "$$\n",
        "where $q_\\phi$ is the approximating variational distribution (inference model). $\\beta$ is the additional annealing parameter that we control. The objective of the entire dataset is the average over all the users. It can be trained almost the same as Multi-DAE, thanks to reparametrization trick. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94iVFyI_KbId"
      },
      "source": [
        "class MultiVAE(MultiDAE):\n",
        "\n",
        "    def construct_placeholders(self):\n",
        "        super(MultiVAE, self).construct_placeholders()\n",
        "\n",
        "        # placeholders with default values when scoring\n",
        "        self.is_training_ph = tf.placeholder_with_default(0., shape=None)\n",
        "        self.anneal_ph = tf.placeholder_with_default(1., shape=None)\n",
        "        \n",
        "    def build_graph(self):\n",
        "        self._construct_weights()\n",
        "\n",
        "        saver, logits, KL = self.forward_pass()\n",
        "        log_softmax_var = tf.nn.log_softmax(logits)\n",
        "\n",
        "        neg_ll = -tf.reduce_mean(tf.reduce_sum(\n",
        "            log_softmax_var * self.input_ph,\n",
        "            axis=-1))\n",
        "        # apply regularization to weights\n",
        "        reg = l2_regularizer(self.lam)\n",
        "        reg_var = apply_regularization(reg, self.weights_q + self.weights_p)\n",
        "        # tensorflow l2 regularization multiply 0.5 to the l2 norm\n",
        "        # multiply 2 so that it is back in the same scale\n",
        "        neg_ELBO = neg_ll + self.anneal_ph * KL + 2 * reg_var\n",
        "        \n",
        "        train_op = tf.train.AdamOptimizer(self.lr).minimize(neg_ELBO)\n",
        "\n",
        "        # add summary statistics\n",
        "        tf.summary.scalar('negative_multi_ll', neg_ll)\n",
        "        tf.summary.scalar('KL', KL)\n",
        "        tf.summary.scalar('neg_ELBO_train', neg_ELBO)\n",
        "        merged = tf.summary.merge_all()\n",
        "\n",
        "        return saver, logits, neg_ELBO, train_op, merged\n",
        "    \n",
        "    def q_graph(self):\n",
        "        mu_q, std_q, KL = None, None, None\n",
        "        \n",
        "        h = tf.nn.l2_normalize(self.input_ph, 1)\n",
        "        h = tf.nn.dropout(h, self.keep_prob_ph)\n",
        "        \n",
        "        for i, (w, b) in enumerate(zip(self.weights_q, self.biases_q)):\n",
        "            h = tf.matmul(h, w) + b\n",
        "            \n",
        "            if i != len(self.weights_q) - 1:\n",
        "                h = tf.nn.tanh(h)\n",
        "            else:\n",
        "                mu_q = h[:, :self.q_dims[-1]]\n",
        "                logvar_q = h[:, self.q_dims[-1]:]\n",
        "\n",
        "                std_q = tf.exp(0.5 * logvar_q)\n",
        "                KL = tf.reduce_mean(tf.reduce_sum(\n",
        "                        0.5 * (-logvar_q + tf.exp(logvar_q) + mu_q**2 - 1), axis=1))\n",
        "        return mu_q, std_q, KL\n",
        "\n",
        "    def p_graph(self, z):\n",
        "        h = z\n",
        "        \n",
        "        for i, (w, b) in enumerate(zip(self.weights_p, self.biases_p)):\n",
        "            h = tf.matmul(h, w) + b\n",
        "            \n",
        "            if i != len(self.weights_p) - 1:\n",
        "                h = tf.nn.tanh(h)\n",
        "        return h\n",
        "\n",
        "    def forward_pass(self):\n",
        "        # q-network\n",
        "        mu_q, std_q, KL = self.q_graph()\n",
        "        epsilon = tf.random_normal(tf.shape(std_q))\n",
        "\n",
        "        sampled_z = mu_q + self.is_training_ph *\\\n",
        "            epsilon * std_q\n",
        "\n",
        "        # p-network\n",
        "        logits = self.p_graph(sampled_z)\n",
        "        \n",
        "        return tf.train.Saver(), logits, KL\n",
        "\n",
        "    def _construct_weights(self):\n",
        "        self.weights_q, self.biases_q = [], []\n",
        "        \n",
        "        for i, (d_in, d_out) in enumerate(zip(self.q_dims[:-1], self.q_dims[1:])):\n",
        "            if i == len(self.q_dims[:-1]) - 1:\n",
        "                # we need two sets of parameters for mean and variance,\n",
        "                # respectively\n",
        "                d_out *= 2\n",
        "            weight_key = \"weight_q_{}to{}\".format(i, i+1)\n",
        "            bias_key = \"bias_q_{}\".format(i+1)\n",
        "            \n",
        "            self.weights_q.append(tf.get_variable(\n",
        "                name=weight_key, shape=[d_in, d_out],\n",
        "                initializer=tf.contrib.layers.xavier_initializer(\n",
        "                    seed=self.random_seed)))\n",
        "            \n",
        "            self.biases_q.append(tf.get_variable(\n",
        "                name=bias_key, shape=[d_out],\n",
        "                initializer=tf.truncated_normal_initializer(\n",
        "                    stddev=0.001, seed=self.random_seed)))\n",
        "            \n",
        "            # add summary stats\n",
        "            tf.summary.histogram(weight_key, self.weights_q[-1])\n",
        "            tf.summary.histogram(bias_key, self.biases_q[-1])\n",
        "            \n",
        "        self.weights_p, self.biases_p = [], []\n",
        "\n",
        "        for i, (d_in, d_out) in enumerate(zip(self.p_dims[:-1], self.p_dims[1:])):\n",
        "            weight_key = \"weight_p_{}to{}\".format(i, i+1)\n",
        "            bias_key = \"bias_p_{}\".format(i+1)\n",
        "            self.weights_p.append(tf.get_variable(\n",
        "                name=weight_key, shape=[d_in, d_out],\n",
        "                initializer=tf.contrib.layers.xavier_initializer(\n",
        "                    seed=self.random_seed)))\n",
        "            \n",
        "            self.biases_p.append(tf.get_variable(\n",
        "                name=bias_key, shape=[d_out],\n",
        "                initializer=tf.truncated_normal_initializer(\n",
        "                    stddev=0.001, seed=self.random_seed)))\n",
        "            \n",
        "            # add summary stats\n",
        "            tf.summary.histogram(weight_key, self.weights_p[-1])\n",
        "            tf.summary.histogram(bias_key, self.biases_p[-1])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhsnuTDKbId"
      },
      "source": [
        "### Training/validation data, hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHeuM78SKbId"
      },
      "source": [
        "Load the pre-processed training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzi4hI4uKbId"
      },
      "source": [
        "unique_sid = list()\n",
        "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'r') as f:\n",
        "    for line in f:\n",
        "        unique_sid.append(line.strip())\n",
        "\n",
        "n_items = len(unique_sid)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nrgFBSQKbId"
      },
      "source": [
        "def load_train_data(csv_file):\n",
        "    tp = pd.read_csv(csv_file)\n",
        "    n_users = tp['uid'].max() + 1\n",
        "\n",
        "    rows, cols = tp['uid'], tp['sid']\n",
        "    data = sparse.csr_matrix((np.ones_like(rows),\n",
        "                             (rows, cols)), dtype='float64',\n",
        "                             shape=(n_users, n_items))\n",
        "    return data"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVN2K0N5KbId"
      },
      "source": [
        "train_data = load_train_data(os.path.join(pro_dir, 'train.csv'))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1zbIgdXKbId"
      },
      "source": [
        "def load_tr_te_data(csv_file_tr, csv_file_te):\n",
        "    tp_tr = pd.read_csv(csv_file_tr)\n",
        "    tp_te = pd.read_csv(csv_file_te)\n",
        "\n",
        "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
        "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
        "\n",
        "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
        "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
        "\n",
        "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
        "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
        "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
        "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
        "    return data_tr, data_te"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWW9jkv8KbId"
      },
      "source": [
        "vad_data_tr, vad_data_te = load_tr_te_data(os.path.join(pro_dir, 'validation_tr.csv'),\n",
        "                                           os.path.join(pro_dir, 'validation_te.csv'))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ2uu1M_KbId"
      },
      "source": [
        "Set up training hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epbUOpjQKbId"
      },
      "source": [
        "N = train_data.shape[0]\n",
        "idxlist = list(range(N))\n",
        "\n",
        "# training batch size\n",
        "batch_size = 500\n",
        "batches_per_epoch = int(np.ceil(float(N) / batch_size))\n",
        "\n",
        "N_vad = vad_data_tr.shape[0]\n",
        "idxlist_vad = list(range(N_vad))\n",
        "\n",
        "# validation batch size (since the entire validation set might not fit into GPU memory)\n",
        "batch_size_vad = 2000\n",
        "\n",
        "# the total number of gradient updates for annealing\n",
        "total_anneal_steps = 200000\n",
        "# largest annealing parameter\n",
        "anneal_cap = 0.2"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe8WpfC4KbId"
      },
      "source": [
        "Evaluate function: Normalized discounted cumulative gain (NDCG@k) and Recall@k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpg3DDfBKbId"
      },
      "source": [
        "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
        "    '''\n",
        "    normalized discounted cumulative gain@k for binary relevance\n",
        "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
        "    '''\n",
        "    batch_users = X_pred.shape[0]\n",
        "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
        "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
        "                       idx_topk_part[:, :k]]\n",
        "    idx_part = np.argsort(-topk_part, axis=1)\n",
        "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
        "    # topk predicted score\n",
        "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
        "    # build the discount template\n",
        "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
        "\n",
        "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
        "                         idx_topk].toarray() * tp).sum(axis=1)\n",
        "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
        "                     for n in heldout_batch.getnnz(axis=1)])\n",
        "    return DCG / IDCG"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvMW1xduKbId"
      },
      "source": [
        "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
        "    batch_users = X_pred.shape[0]\n",
        "\n",
        "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
        "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
        "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
        "\n",
        "    X_true_binary = (heldout_batch > 0).toarray()\n",
        "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
        "        np.float32)\n",
        "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
        "    return recall"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KklHu46iKbId"
      },
      "source": [
        "### Train a Multi-VAE^{PR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIxPZcRwKbId"
      },
      "source": [
        "For ML-20M dataset, we set both the generative function $f_\\theta(\\cdot)$ and the inference model $g_\\phi(\\cdot)$ to be 3-layer multilayer perceptron (MLP) with symmetrical architecture. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txnn1fc1KbId"
      },
      "source": [
        "The generative function is a [200 -> 600 -> n_items] MLP, which means the inference function is a [n_items -> 600 -> 200] MLP. Thus the overall architecture for the Multi-VAE^{PR} is [n_items -> 600 -> 200 -> 600 -> n_items]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zguWFBo8KbIe"
      },
      "source": [
        "p_dims = [200, 600, n_items]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJkeTnt4KbIe",
        "outputId": "3a6e946b-6e9a-4268-9ae3-b7a050710087"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "vae = MultiVAE(p_dims, lam=0.0, random_seed=98765)\n",
        "\n",
        "saver, logits_var, loss_var, train_op_var, merged_var = vae.build_graph()\n",
        "\n",
        "ndcg_var = tf.Variable(0.0)\n",
        "ndcg_dist_var = tf.placeholder(dtype=tf.float64, shape=None)\n",
        "ndcg_summary = tf.summary.scalar('ndcg_at_k_validation', ndcg_var)\n",
        "ndcg_dist_summary = tf.summary.histogram('ndcg_at_k_hist_validation', ndcg_dist_var)\n",
        "merged_valid = tf.summary.merge([ndcg_summary, ndcg_dist_summary])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-33-238c8eded668>:40: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5U-tOm1KbIe"
      },
      "source": [
        "Set up logging and checkpoint directory\n",
        "\n",
        "- Change all the logging directory and checkpoint directory to somewhere of your choice\n",
        "- Monitor training progress using tensorflow by: `tensorboard --logdir=$log_dir`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYqzMPB0KbIe"
      },
      "source": [
        "arch_str = \"I-%s-I\" % ('-'.join([str(d) for d in vae.dims[1:-1]]))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_6LvF7JKbIe",
        "outputId": "bec40781-793b-4265-bdb7-856fa9c11f9b"
      },
      "source": [
        "log_dir = '/content/drive/My Drive/Study/Project3/ml-10m/log/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
        "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
        "\n",
        "if os.path.exists(log_dir):\n",
        "    shutil.rmtree(log_dir)\n",
        "\n",
        "print(\"log directory: %s\" % log_dir)\n",
        "summary_writer = tf.summary.FileWriter(log_dir, graph=tf.get_default_graph())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log directory: /content/drive/My Drive/Study/Project3/ml-10m/log/VAE_anneal200.0K_cap2.0E-01/I-600-200-600-I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtRExyrSKbIe",
        "outputId": "c3f62870-cd77-4931-9180-47377bdfe1b2"
      },
      "source": [
        "chkpt_dir = '/content/drive/My Drive/Study/Project3/ml-10m/checkpoints/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
        "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
        "\n",
        "if not os.path.isdir(chkpt_dir):\n",
        "    os.makedirs(chkpt_dir) \n",
        "    \n",
        "print(\"chkpt directory: %s\" % chkpt_dir)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chkpt directory: /content/drive/My Drive/Study/Project3/ml-10m/checkpoints/VAE_anneal200.0K_cap2.0E-01/I-600-200-600-I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSJJOgKJKbIf"
      },
      "source": [
        "n_epochs = 150"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGnvfwA-KbIf"
      },
      "source": [
        "ndcgs_vad = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "\n",
        "    best_ndcg = -np.inf\n",
        "\n",
        "    update_count = 0.0\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        np.random.shuffle(idxlist)\n",
        "        # train for one epoch\n",
        "        for bnum, st_idx in enumerate(range(0, N, batch_size)):\n",
        "            end_idx = min(st_idx + batch_size, N)\n",
        "            X = train_data[idxlist[st_idx:end_idx]]\n",
        "            \n",
        "            if sparse.isspmatrix(X):\n",
        "                X = X.toarray()\n",
        "            X = X.astype('float32')           \n",
        "            \n",
        "            if total_anneal_steps > 0:\n",
        "                anneal = min(anneal_cap, 1. * update_count / total_anneal_steps)\n",
        "            else:\n",
        "                anneal = anneal_cap\n",
        "            \n",
        "            feed_dict = {vae.input_ph: X, \n",
        "                         vae.keep_prob_ph: 0.5, \n",
        "                         vae.anneal_ph: anneal,\n",
        "                         vae.is_training_ph: 1}        \n",
        "            sess.run(train_op_var, feed_dict=feed_dict)\n",
        "\n",
        "            if bnum % 100 == 0:\n",
        "                summary_train = sess.run(merged_var, feed_dict=feed_dict)\n",
        "                summary_writer.add_summary(summary_train, \n",
        "                                           global_step=epoch * batches_per_epoch + bnum) \n",
        "            \n",
        "            update_count += 1\n",
        "        \n",
        "        # compute validation NDCG\n",
        "        ndcg_dist = []\n",
        "        for bnum, st_idx in enumerate(range(0, N_vad, batch_size_vad)):\n",
        "            end_idx = min(st_idx + batch_size_vad, N_vad)\n",
        "            X = vad_data_tr[idxlist_vad[st_idx:end_idx]]\n",
        "\n",
        "            if sparse.isspmatrix(X):\n",
        "                X = X.toarray()\n",
        "            X = X.astype('float32')\n",
        "        \n",
        "            pred_val = sess.run(logits_var, feed_dict={vae.input_ph: X} )\n",
        "            # exclude examples from training and validation (if any)\n",
        "            pred_val[X.nonzero()] = -np.inf\n",
        "            ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, vad_data_te[idxlist_vad[st_idx:end_idx]]))\n",
        "        \n",
        "        ndcg_dist = np.concatenate(ndcg_dist)\n",
        "        ndcg_ = ndcg_dist.mean()\n",
        "        ndcgs_vad.append(ndcg_)\n",
        "        merged_valid_val = sess.run(merged_valid, feed_dict={ndcg_var: ndcg_, ndcg_dist_var: ndcg_dist})\n",
        "        summary_writer.add_summary(merged_valid_val, epoch)\n",
        "\n",
        "        # update the best model (if necessary)\n",
        "        if ndcg_ > best_ndcg:\n",
        "            saver.save(sess, '{}/model'.format(chkpt_dir))\n",
        "            best_ndcg = ndcg_"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YxiD4pwKbIf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "d1a3881e-03cb-4953-8b91-865ee999d632"
      },
      "source": [
        "plt.figure(figsize=(12, 3))\n",
        "plt.plot(ndcgs_vad)\n",
        "plt.ylabel(\"Validation NDCG@100\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "pass"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMdqcRMWKbIf"
      },
      "source": [
        "### Load the test data and compute test metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wOALoUOKbIf"
      },
      "source": [
        "test_data_tr, test_data_te = load_tr_te_data(\n",
        "    os.path.join(pro_dir, 'test_tr.csv'),\n",
        "    os.path.join(pro_dir, 'test_te.csv'))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bJdNtXpKbIf"
      },
      "source": [
        "N_test = test_data_tr.shape[0]\n",
        "idxlist_test = range(N_test)\n",
        "\n",
        "batch_size_test = 2000"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z14AeVaKbIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f79162ed-7db0-4ba5-e109-20ec94ad0f46"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "vae = MultiVAE(p_dims, lam=0.0)\n",
        "saver, logits_var, _, _, _ = vae.build_graph()    "
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Scale of 0 disables regularizer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IfeVohUKbIf"
      },
      "source": [
        "Load the best performing model on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axVRJgY-KbIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df5aeec-b6c0-409c-be70-b11993864286"
      },
      "source": [
        "chkpt_dir = '/content/drive/My Drive/Study/Project3/ml-10m/checkpoints/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
        "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
        "print(\"chkpt directory: %s\" % chkpt_dir)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chkpt directory: /content/drive/My Drive/Study/Project3/ml-10m/checkpoints/VAE_anneal200.0K_cap2.0E-01/I-600-200-600-I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR3wWxPvKbIg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc55566f-43a5-460a-a5e5-f55490d00f39"
      },
      "source": [
        "n100_list, r20_list, r50_list = [], [], []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, '{}/model'.format(chkpt_dir))\n",
        "\n",
        "    for bnum, st_idx in enumerate(range(0, N_test, batch_size_test)):\n",
        "        end_idx = min(st_idx + batch_size_test, N_test)\n",
        "        X = test_data_tr[idxlist_test[st_idx:end_idx]]\n",
        "\n",
        "        if sparse.isspmatrix(X):\n",
        "            X = X.toarray()\n",
        "        X = X.astype('float32')\n",
        "\n",
        "        pred_val = sess.run(logits_var, feed_dict={vae.input_ph: X})\n",
        "        # exclude examples from training and validation (if any)\n",
        "        pred_val[X.nonzero()] = -np.inf\n",
        "        n100_list.append(NDCG_binary_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=100))\n",
        "        r20_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=20))\n",
        "        r50_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=50))\n",
        "    \n",
        "n100_list = np.concatenate(n100_list)\n",
        "r20_list = np.concatenate(r20_list)\n",
        "r50_list = np.concatenate(r50_list)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Study/Project3/ml-10m/checkpoints/VAE_anneal200.0K_cap2.0E-01/I-600-200-600-I/model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQW1bzwkKbIg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c630f31-00ad-4db6-d189-30bcc9619a80"
      },
      "source": [
        "print(\"Test NDCG@100=%.5f (%.5f)\" % (np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))))\n",
        "print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
        "print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list), np.std(r50_list) / np.sqrt(len(r50_list))))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test NDCG@100=0.43143 (0.00250)\n",
            "Test Recall@20=0.40319 (0.00330)\n",
            "Test Recall@50=0.55083 (0.00347)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}