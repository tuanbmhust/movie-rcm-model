{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WhQ8g-UDLpqn",
    "outputId": "ed489bee-9e10-4a32-98fe-28090e38e7e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8y-MU2H5KbIZ"
   },
   "source": [
    "# Variational autoencoders for collaborative filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBvM4FolKbIZ"
   },
   "source": [
    "This notebook accompanies the paper \"*Variational autoencoders for collaborative filtering*\" by Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara, in The Web Conference (aka WWW) 2018.\n",
    "\n",
    "In this notebook, we will show a complete self-contained example of training a variational autoencoder (as well as a denoising autoencoder) with multinomial likelihood (described in the paper) on the public Movielens-20M dataset, including both data preprocessing and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XqHVSXvVKbIa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sn\n",
    "sn.set()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import apply_regularization, l2_regularizer\n",
    "import bottleneck as bn\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jSKlwM2KbIa"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FfmcWszKbIa"
   },
   "source": [
    "We load the data and create train/validation/test splits following strong generalization: \n",
    "\n",
    "- We split all users into training/validation/test sets. \n",
    "\n",
    "- We train models using the entire click history of the training users. \n",
    "\n",
    "- To evaluate, we take part of the click history from held-out (validation and test) users to learn the necessary user-level representations for the model and then compute metrics by looking at how well the model ranks the rest of the unseen click history from the held-out users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXxRDM7NKbIa"
   },
   "source": [
    "First, download the dataset at http://files.grouplens.org/datasets/movielens/ml-25m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k6aQkgc3KbIa",
    "outputId": "a3747824-51fc-4def-efe6-701e698e988c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')\n",
    "### change `DATA_DIR` to the location where movielens-20m dataset sits\n",
    "DATA_DIR = '/content/drive/My Drive/Study/Project3/ml-25m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6qBQWbvSKbIa"
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'ratings.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Fq7xOWy9KbIa"
   },
   "outputs": [],
   "source": [
    "# binarize the data (only keep ratings >= 4)\n",
    "raw_data = raw_data[raw_data['rating'] > 3.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "C_YwUsZwKbIa",
    "outputId": "f0e200bd-4da8-450e-99e2-31143ce654e6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147880044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147868828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147878820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1088</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1147868495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1237</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147868839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1      296     5.0  1147880044\n",
       "2       1      307     5.0  1147868828\n",
       "3       1      665     5.0  1147878820\n",
       "5       1     1088     4.0  1147868495\n",
       "8       1     1237     5.0  1147868839"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75JhuEB_KbIb"
   },
   "source": [
    "### Data splitting procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdOzp4RfKbIb"
   },
   "source": [
    "- Select 10K users as heldout users, 10K users as validation users, and the rest of the users for training\n",
    "- Use all the items from the training users as item set\n",
    "- For each of both validation and test user, subsample 80% as fold-in data and the rest for prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ChLy_ymAKbIb"
   },
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tlcITsDZKbIb"
   },
   "outputs": [],
   "source": [
    "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
    "    # Only keep the triplets for items which were clicked on by at least min_sc users. \n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'movieId')\n",
    "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "    \n",
    "    # Only keep the triplets for users who clicked on at least min_uc items\n",
    "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'userId')\n",
    "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
    "    \n",
    "    # Update both usercount and itemcount after filtering\n",
    "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId') \n",
    "    return tp, usercount, itemcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RYQPeH2KbIb"
   },
   "source": [
    "Only keep items that are clicked on by at least 5 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AQGWw184KbIb"
   },
   "outputs": [],
   "source": [
    "raw_data, user_activity, item_popularity = filter_triplets(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E9-rt2ezKbIb",
    "outputId": "47243ab8-d4b3-47e4-ec24-7943ec08eafe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 12448242 watching events from 160776 users and 40857 movies (sparsity: 0.190%)\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1. * raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
    "\n",
    "print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" % \n",
    "      (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FbyWcf1pKbIb"
   },
   "outputs": [],
   "source": [
    "unique_uid = user_activity.index\n",
    "\n",
    "np.random.seed(98765)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uTku1H1DKbIb"
   },
   "outputs": [],
   "source": [
    "# create train/validation/test users\n",
    "n_users = unique_uid.size\n",
    "n_heldout_users = 10000\n",
    "\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bdASvG79KbIb"
   },
   "outputs": [],
   "source": [
    "train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ne0HiGtOKbIb"
   },
   "outputs": [],
   "source": [
    "unique_sid = pd.unique(train_plays['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6Av46YOzKbIc"
   },
   "outputs": [],
   "source": [
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "v8MYrWjdKbIc"
   },
   "outputs": [],
   "source": [
    "# pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
    "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
    "\n",
    "if not os.path.exists(pro_dir):\n",
    "    os.makedirs(pro_dir)\n",
    "\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fW7cBfvNKbIc"
   },
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('userId')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "\n",
    "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"%d users sampled\" % i)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    \n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "MLuS5LXwKbIc"
   },
   "outputs": [],
   "source": [
    "vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pCxg4OUKbIc",
    "outputId": "138c055b-e87b-403f-b5aa-73af96affb0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TSpz_DX0KbIc"
   },
   "outputs": [],
   "source": [
    "test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0q_LYlJFKbIc",
    "outputId": "6a0d3a84-6976-482f-946e-bc2f224ec513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D12r0whSKbIc"
   },
   "source": [
    "### Save the data into (user_index, item_index) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "2F_kXEokKbIc"
   },
   "outputs": [],
   "source": [
    "def numerize(tp):\n",
    "    uid = list(map(lambda x: profile2id[x], tp['userId']))\n",
    "    sid = list(map(lambda x: show2id[x], tp['movieId']))\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "GUHPsKhZKbIc"
   },
   "outputs": [],
   "source": [
    "train_data = numerize(train_plays)\n",
    "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "IRNSTf0QKbIc"
   },
   "outputs": [],
   "source": [
    "vad_data_tr = numerize(vad_plays_tr)\n",
    "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "O4o0GVKaKbIc"
   },
   "outputs": [],
   "source": [
    "vad_data_te = numerize(vad_plays_te)\n",
    "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "8KBW_3aqKbIc"
   },
   "outputs": [],
   "source": [
    "test_data_tr = numerize(test_plays_tr)\n",
    "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "WdB3PRQJKbIc"
   },
   "outputs": [],
   "source": [
    "test_data_te = numerize(test_plays_te)\n",
    "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McZLq8QkKbIc"
   },
   "source": [
    "## Model definition and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIlMAfz4KbId"
   },
   "source": [
    "We define two related models: denoising autoencoder with multinomial likelihood (Multi-DAE in the paper) and partially-regularized variational autoencoder with multinomial likelihood (Multi-VAE^{PR} in the paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCPIIYYMKbId"
   },
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEcLKHvcKbId"
   },
   "source": [
    "__Notations__: We use $u \\in \\{1,\\dots,U\\}$ to index users and $i \\in \\{1,\\dots,I\\}$ to index items. In this work, we consider learning with implicit feedback. The user-by-item interaction matrix is the click matrix $\\mathbf{X} \\in \\mathbb{N}^{U\\times I}$. The lower case $\\mathbf{x}_u =[X_{u1},\\dots,X_{uI}]^\\top \\in \\mathbb{N}^I$ is a bag-of-words vector with the number of clicks for each item from user u. We binarize the click matrix. It is straightforward to extend it to general count data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvW4hHdrKbId"
   },
   "source": [
    "__Generative process__: For each user $u$, the model starts by sampling a $K$-dimensional latent representation $\\mathbf{z}_u$ from a standard Gaussian prior. The latent representation $\\mathbf{z}_u$ is transformed via a non-linear function $f_\\theta (\\cdot) \\in \\mathbb{R}^I$ to produce a probability distribution over $I$ items $\\pi (\\mathbf{z}_u)$ from which the click history $\\mathbf{x}_u$ is assumed to have been drawn:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_u \\sim \\mathcal{N}(0, \\mathbf{I}_K),  \\pi(\\mathbf{z}_u) \\propto \\exp\\{f_\\theta (\\mathbf{z}_u\\},\\\\\n",
    "\\mathbf{x}_u \\sim \\mathrm{Mult}(N_u, \\pi(\\mathbf{z}_u))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dIsvORCKbId"
   },
   "source": [
    "The objective for Multi-DAE for a single user $u$ is:\n",
    "$$\n",
    "\\mathcal{L}_u(\\theta, \\phi) = \\log p_\\theta(\\mathbf{x}_u | g_\\phi(\\mathbf{x}_u))\n",
    "$$\n",
    "where $g_\\phi(\\cdot)$ is the non-linear \"encoder\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "cy2LESs4KbId"
   },
   "outputs": [],
   "source": [
    "class MultiDAE(object):\n",
    "    def __init__(self, p_dims, q_dims=None, lam=0.01, lr=1e-3, random_seed=None):\n",
    "        self.p_dims = p_dims\n",
    "        if q_dims is None:\n",
    "            self.q_dims = p_dims[::-1]\n",
    "        else:\n",
    "            assert q_dims[0] == p_dims[-1], \"Input and output dimension must equal each other for autoencoders.\"\n",
    "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q-network mismatches.\"\n",
    "            self.q_dims = q_dims\n",
    "        self.dims = self.q_dims + self.p_dims[1:]\n",
    "        \n",
    "        self.lam = lam\n",
    "        self.lr = lr\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        self.construct_placeholders()\n",
    "\n",
    "    def construct_placeholders(self):        \n",
    "        self.input_ph = tf.placeholder(\n",
    "            dtype=tf.float32, shape=[None, self.dims[0]])\n",
    "        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=None)\n",
    "\n",
    "    def build_graph(self):\n",
    "\n",
    "        self.construct_weights()\n",
    "\n",
    "        saver, logits = self.forward_pass()\n",
    "        log_softmax_var = tf.nn.log_softmax(logits)\n",
    "\n",
    "        # per-user average negative log-likelihood\n",
    "        neg_ll = -tf.reduce_mean(tf.reduce_sum(\n",
    "            log_softmax_var * self.input_ph, axis=1))\n",
    "        # apply regularization to weights\n",
    "        reg = l2_regularizer(self.lam)\n",
    "        reg_var = apply_regularization(reg, self.weights)\n",
    "        # tensorflow l2 regularization multiply 0.5 to the l2 norm\n",
    "        # multiply 2 so that it is back in the same scale\n",
    "        loss = neg_ll + 2 * reg_var\n",
    "        \n",
    "        train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\n",
    "\n",
    "        # add summary statistics\n",
    "        tf.summary.scalar('negative_multi_ll', neg_ll)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        merged = tf.summary.merge_all()\n",
    "        return saver, logits, loss, train_op, merged\n",
    "\n",
    "    def forward_pass(self):\n",
    "        # construct forward graph        \n",
    "        h = tf.nn.l2_normalize(self.input_ph, 1)\n",
    "        h = tf.nn.dropout(h, self.keep_prob_ph)\n",
    "        \n",
    "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "            \n",
    "            if i != len(self.weights) - 1:\n",
    "                h = tf.nn.tanh(h)\n",
    "        return tf.train.Saver(), h\n",
    "\n",
    "    def construct_weights(self):\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        # define weights\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.dims[:-1], self.dims[1:])):\n",
    "            weight_key = \"weight_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_{}\".format(i+1)\n",
    "            \n",
    "            self.weights.append(tf.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "            self.biases.append(tf.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "            \n",
    "            # add summary stats\n",
    "            tf.summary.histogram(weight_key, self.weights[-1])\n",
    "            tf.summary.histogram(bias_key, self.biases[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-P1xXakVKbId"
   },
   "source": [
    "The objective of Multi-VAE^{PR} (evidence lower-bound, or ELBO) for a single user $u$ is:\n",
    "$$\n",
    "\\mathcal{L}_u(\\theta, \\phi) = \\mathbb{E}_{q_\\phi(z_u | x_u)}[\\log p_\\theta(x_u | z_u)] - \\beta \\cdot KL(q_\\phi(z_u | x_u) \\| p(z_u))\n",
    "$$\n",
    "where $q_\\phi$ is the approximating variational distribution (inference model). $\\beta$ is the additional annealing parameter that we control. The objective of the entire dataset is the average over all the users. It can be trained almost the same as Multi-DAE, thanks to reparametrization trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "94iVFyI_KbId"
   },
   "outputs": [],
   "source": [
    "class MultiVAE(MultiDAE):\n",
    "\n",
    "    def construct_placeholders(self):\n",
    "        super(MultiVAE, self).construct_placeholders()\n",
    "\n",
    "        # placeholders with default values when scoring\n",
    "        self.is_training_ph = tf.placeholder_with_default(0., shape=None)\n",
    "        self.anneal_ph = tf.placeholder_with_default(1., shape=None)\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self._construct_weights()\n",
    "\n",
    "        saver, logits, KL = self.forward_pass()\n",
    "        log_softmax_var = tf.nn.log_softmax(logits)\n",
    "\n",
    "        neg_ll = -tf.reduce_mean(tf.reduce_sum(\n",
    "            log_softmax_var * self.input_ph,\n",
    "            axis=-1))\n",
    "        # apply regularization to weights\n",
    "        reg = l2_regularizer(self.lam)\n",
    "        reg_var = apply_regularization(reg, self.weights_q + self.weights_p)\n",
    "        # tensorflow l2 regularization multiply 0.5 to the l2 norm\n",
    "        # multiply 2 so that it is back in the same scale\n",
    "        neg_ELBO = neg_ll + self.anneal_ph * KL + 2 * reg_var\n",
    "        \n",
    "        train_op = tf.train.AdamOptimizer(self.lr).minimize(neg_ELBO)\n",
    "\n",
    "        # add summary statistics\n",
    "        tf.summary.scalar('negative_multi_ll', neg_ll)\n",
    "        tf.summary.scalar('KL', KL)\n",
    "        tf.summary.scalar('neg_ELBO_train', neg_ELBO)\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        return saver, logits, neg_ELBO, train_op, merged\n",
    "    \n",
    "    def q_graph(self):\n",
    "        mu_q, std_q, KL = None, None, None\n",
    "        \n",
    "        h = tf.nn.l2_normalize(self.input_ph, 1)\n",
    "        h = tf.nn.dropout(h, self.keep_prob_ph)\n",
    "        \n",
    "        for i, (w, b) in enumerate(zip(self.weights_q, self.biases_q)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "            \n",
    "            if i != len(self.weights_q) - 1:\n",
    "                h = tf.nn.tanh(h)\n",
    "            else:\n",
    "                mu_q = h[:, :self.q_dims[-1]]\n",
    "                logvar_q = h[:, self.q_dims[-1]:]\n",
    "\n",
    "                std_q = tf.exp(0.5 * logvar_q)\n",
    "                KL = tf.reduce_mean(tf.reduce_sum(\n",
    "                        0.5 * (-logvar_q + tf.exp(logvar_q) + mu_q**2 - 1), axis=1))\n",
    "        return mu_q, std_q, KL\n",
    "\n",
    "    def p_graph(self, z):\n",
    "        h = z\n",
    "        \n",
    "        for i, (w, b) in enumerate(zip(self.weights_p, self.biases_p)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "            \n",
    "            if i != len(self.weights_p) - 1:\n",
    "                h = tf.nn.tanh(h)\n",
    "        return h\n",
    "\n",
    "    def forward_pass(self):\n",
    "        # q-network\n",
    "        mu_q, std_q, KL = self.q_graph()\n",
    "        epsilon = tf.random_normal(tf.shape(std_q))\n",
    "\n",
    "        sampled_z = mu_q + self.is_training_ph *\\\n",
    "            epsilon * std_q\n",
    "\n",
    "        # p-network\n",
    "        logits = self.p_graph(sampled_z)\n",
    "        \n",
    "        return tf.train.Saver(), logits, KL\n",
    "\n",
    "    def _construct_weights(self):\n",
    "        self.weights_q, self.biases_q = [], []\n",
    "        \n",
    "        for i, (d_in, d_out) in enumerate(zip(self.q_dims[:-1], self.q_dims[1:])):\n",
    "            if i == len(self.q_dims[:-1]) - 1:\n",
    "                # we need two sets of parameters for mean and variance,\n",
    "                # respectively\n",
    "                d_out *= 2\n",
    "            weight_key = \"weight_q_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_q_{}\".format(i+1)\n",
    "            \n",
    "            self.weights_q.append(tf.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "            self.biases_q.append(tf.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "            \n",
    "            # add summary stats\n",
    "            tf.summary.histogram(weight_key, self.weights_q[-1])\n",
    "            tf.summary.histogram(bias_key, self.biases_q[-1])\n",
    "            \n",
    "        self.weights_p, self.biases_p = [], []\n",
    "\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.p_dims[:-1], self.p_dims[1:])):\n",
    "            weight_key = \"weight_p_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_p_{}\".format(i+1)\n",
    "            self.weights_p.append(tf.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "            self.biases_p.append(tf.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "            \n",
    "            # add summary stats\n",
    "            tf.summary.histogram(weight_key, self.weights_p[-1])\n",
    "            tf.summary.histogram(bias_key, self.biases_p[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxhsnuTDKbId"
   },
   "source": [
    "### Training/validation data, hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHeuM78SKbId"
   },
   "source": [
    "Load the pre-processed training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "lzi4hI4uKbId"
   },
   "outputs": [],
   "source": [
    "unique_sid = list()\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        unique_sid.append(line.strip())\n",
    "\n",
    "n_items = len(unique_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7nrgFBSQKbId"
   },
   "outputs": [],
   "source": [
    "def load_train_data(csv_file):\n",
    "    tp = pd.read_csv(csv_file)\n",
    "    n_users = tp['uid'].max() + 1\n",
    "\n",
    "    rows, cols = tp['uid'], tp['sid']\n",
    "    data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                             (rows, cols)), dtype='float64',\n",
    "                             shape=(n_users, n_items))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "DVN2K0N5KbId"
   },
   "outputs": [],
   "source": [
    "train_data = load_train_data(os.path.join(pro_dir, 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "n1zbIgdXKbId"
   },
   "outputs": [],
   "source": [
    "def load_tr_te_data(csv_file_tr, csv_file_te):\n",
    "    tp_tr = pd.read_csv(csv_file_tr)\n",
    "    tp_te = pd.read_csv(csv_file_te)\n",
    "\n",
    "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "CWW9jkv8KbId"
   },
   "outputs": [],
   "source": [
    "vad_data_tr, vad_data_te = load_tr_te_data(os.path.join(pro_dir, 'validation_tr.csv'),\n",
    "                                           os.path.join(pro_dir, 'validation_te.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ2uu1M_KbId"
   },
   "source": [
    "Set up training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "epbUOpjQKbId"
   },
   "outputs": [],
   "source": [
    "N = train_data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "# training batch size\n",
    "batch_size = 500\n",
    "batches_per_epoch = int(np.ceil(float(N) / batch_size))\n",
    "\n",
    "N_vad = vad_data_tr.shape[0]\n",
    "idxlist_vad = list(range(N_vad))\n",
    "\n",
    "# validation batch size (since the entire validation set might not fit into GPU memory)\n",
    "batch_size_vad = 2000\n",
    "\n",
    "# the total number of gradient updates for annealing\n",
    "total_anneal_steps = 200000\n",
    "# largest annealing parameter\n",
    "anneal_cap = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qe8WpfC4KbId"
   },
   "source": [
    "Evaluate function: Normalized discounted cumulative gain (NDCG@k) and Recall@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Xpg3DDfBKbId"
   },
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                     for n in heldout_batch.getnnz(axis=1)])\n",
    "    return DCG / IDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "vvMW1xduKbId"
   },
   "outputs": [],
   "source": [
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KklHu46iKbId"
   },
   "source": [
    "### Train a Multi-VAE^{PR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIxPZcRwKbId"
   },
   "source": [
    "For ML-20M dataset, we set both the generative function $f_\\theta(\\cdot)$ and the inference model $g_\\phi(\\cdot)$ to be 3-layer multilayer perceptron (MLP) with symmetrical architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Txnn1fc1KbId"
   },
   "source": [
    "The generative function is a [200 -> 600 -> n_items] MLP, which means the inference function is a [n_items -> 600 -> 200] MLP. Thus the overall architecture for the Multi-VAE^{PR} is [n_items -> 600 -> 200 -> 600 -> n_items]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "zguWFBo8KbIe"
   },
   "outputs": [],
   "source": [
    "p_dims = [200, 600, n_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJkeTnt4KbIe",
    "outputId": "76d4e3a3-617a-4bad-a741-d1f23f2aa090"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-29-238c8eded668>:40: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "vae = MultiVAE(p_dims, lam=0.0, random_seed=98765)\n",
    "\n",
    "saver, logits_var, loss_var, train_op_var, merged_var = vae.build_graph()\n",
    "\n",
    "ndcg_var = tf.Variable(0.0)\n",
    "ndcg_dist_var = tf.placeholder(dtype=tf.float64, shape=None)\n",
    "ndcg_summary = tf.summary.scalar('ndcg_at_k_validation', ndcg_var)\n",
    "ndcg_dist_summary = tf.summary.histogram('ndcg_at_k_hist_validation', ndcg_dist_var)\n",
    "merged_valid = tf.summary.merge([ndcg_summary, ndcg_dist_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5U-tOm1KbIe"
   },
   "source": [
    "Set up logging and checkpoint directory\n",
    "\n",
    "- Change all the logging directory and checkpoint directory to somewhere of your choice\n",
    "- Monitor training progress using tensorflow by: `tensorboard --logdir=$log_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "KYqzMPB0KbIe"
   },
   "outputs": [],
   "source": [
    "arch_str = \"I-%s-I\" % ('-'.join([str(d) for d in vae.dims[1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_6LvF7JKbIe",
    "outputId": "0e8197b7-82e6-413f-eb3d-6f1210d81d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log directory: /content/drive/My Drive/Study/Project3/ml-25m/log/VAE_anneal200.0K_cap2.0E-01/I-600-200-600-I\n"
     ]
    }
   ],
   "source": [
    "log_dir = '/content/drive/My Drive/Study/Project3/ml-25m/log/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
    "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
    "\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "\n",
    "print(\"log directory: %s\" % log_dir)\n",
    "summary_writer = tf.summary.FileWriter(log_dir, graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtRExyrSKbIe",
    "outputId": "6242d69e-6d0e-497c-a26e-357f4bb25d26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chkpt directory: /content/drive/My Drive/Study/Project3/ml-25m/checkpoints/VAE_anneal200.0K_cap2.0E-01/I-600-200-600-I\n"
     ]
    }
   ],
   "source": [
    "chkpt_dir = '/content/drive/My Drive/Study/Project3/ml-25m/checkpoints/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
    "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
    "\n",
    "if not os.path.isdir(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir) \n",
    "    \n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "wSJJOgKJKbIf"
   },
   "outputs": [],
   "source": [
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "gGnvfwA-KbIf"
   },
   "outputs": [],
   "source": [
    "ndcgs_vad = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    best_ndcg = -np.inf\n",
    "\n",
    "    update_count = 0.0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        np.random.shuffle(idxlist)\n",
    "        # train for one epoch\n",
    "        for bnum, st_idx in enumerate(range(0, N, batch_size)):\n",
    "            end_idx = min(st_idx + batch_size, N)\n",
    "            X = train_data[idxlist[st_idx:end_idx]]\n",
    "            \n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')           \n",
    "            \n",
    "            if total_anneal_steps > 0:\n",
    "                anneal = min(anneal_cap, 1. * update_count / total_anneal_steps)\n",
    "            else:\n",
    "                anneal = anneal_cap\n",
    "            \n",
    "            feed_dict = {vae.input_ph: X, \n",
    "                         vae.keep_prob_ph: 0.5, \n",
    "                         vae.anneal_ph: anneal,\n",
    "                         vae.is_training_ph: 1}        \n",
    "            sess.run(train_op_var, feed_dict=feed_dict)\n",
    "\n",
    "            if bnum % 100 == 0:\n",
    "                summary_train = sess.run(merged_var, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_train, \n",
    "                                           global_step=epoch * batches_per_epoch + bnum) \n",
    "            \n",
    "            update_count += 1\n",
    "        \n",
    "        # compute validation NDCG\n",
    "        ndcg_dist = []\n",
    "        for bnum, st_idx in enumerate(range(0, N_vad, batch_size_vad)):\n",
    "            end_idx = min(st_idx + batch_size_vad, N_vad)\n",
    "            X = vad_data_tr[idxlist_vad[st_idx:end_idx]]\n",
    "\n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')\n",
    "        \n",
    "            pred_val = sess.run(logits_var, feed_dict={vae.input_ph: X} )\n",
    "            # exclude examples from training and validation (if any)\n",
    "            pred_val[X.nonzero()] = -np.inf\n",
    "            ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, vad_data_te[idxlist_vad[st_idx:end_idx]]))\n",
    "        \n",
    "        ndcg_dist = np.concatenate(ndcg_dist)\n",
    "        ndcg_ = ndcg_dist.mean()\n",
    "        ndcgs_vad.append(ndcg_)\n",
    "        merged_valid_val = sess.run(merged_valid, feed_dict={ndcg_var: ndcg_, ndcg_dist_var: ndcg_dist})\n",
    "        summary_writer.add_summary(merged_valid_val, epoch)\n",
    "\n",
    "        # update the best model (if necessary)\n",
    "        if ndcg_ > best_ndcg:\n",
    "            saver.save(sess, '{}/model'.format(chkpt_dir))\n",
    "            best_ndcg = ndcg_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "id": "3YxiD4pwKbIf",
    "outputId": "ed3fedde-087b-4879-e66c-d586d362cab4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(ndcgs_vad)\n",
    "plt.ylabel(\"Validation NDCG@100\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMdqcRMWKbIf"
   },
   "source": [
    "### Load the test data and compute test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "8wOALoUOKbIf"
   },
   "outputs": [],
   "source": [
    "test_data_tr, test_data_te = load_tr_te_data(\n",
    "    os.path.join(pro_dir, 'test_tr.csv'),\n",
    "    os.path.join(pro_dir, 'test_te.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "1bJdNtXpKbIf"
   },
   "outputs": [],
   "source": [
    "N_test = test_data_tr.shape[0]\n",
    "idxlist_test = range(N_test)\n",
    "\n",
    "batch_size_test = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Z14AeVaKbIf",
    "outputId": "4ffb720f-f464-4b19-a9b0-379873a89a7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "vae = MultiVAE(p_dims, lam=0.0)\n",
    "saver, logits_var, _, _, _ = vae.build_graph()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IfeVohUKbIf"
   },
   "source": [
    "Load the best performing model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axVRJgY-KbIf",
    "outputId": "1eb19a58-37e7-4b5d-dedd-b7ea6f0198f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chkpt directory: /content/drive/My Drive/Study/Project3/ml-25m/checkpoints/VAE_anneal200.0K_cap2.0E-01/I-600-200-600-I\n"
     ]
    }
   ],
   "source": [
    "chkpt_dir = '/content/drive/My Drive/Study/Project3/ml-25m/checkpoints/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
    "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xR3wWxPvKbIg",
    "outputId": "288fb77f-d13b-4ba0-ab6a-712cd2636e0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Study/Project3/ml-25m/checkpoints/VAE_anneal200.0K_cap2.0E-01/I-600-200-600-I/model\n"
     ]
    }
   ],
   "source": [
    "n100_list, r20_list, r50_list = [], [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, '{}/model'.format(chkpt_dir))\n",
    "\n",
    "    for bnum, st_idx in enumerate(range(0, N_test, batch_size_test)):\n",
    "        end_idx = min(st_idx + batch_size_test, N_test)\n",
    "        X = test_data_tr[idxlist_test[st_idx:end_idx]]\n",
    "\n",
    "        if sparse.isspmatrix(X):\n",
    "            X = X.toarray()\n",
    "        X = X.astype('float32')\n",
    "\n",
    "        pred_val = sess.run(logits_var, feed_dict={vae.input_ph: X})\n",
    "        # exclude examples from training and validation (if any)\n",
    "        pred_val[X.nonzero()] = -np.inf\n",
    "        n100_list.append(NDCG_binary_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=100))\n",
    "        r20_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=20))\n",
    "        r50_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=50))\n",
    "    \n",
    "n100_list = np.concatenate(n100_list)\n",
    "r20_list = np.concatenate(r20_list)\n",
    "r50_list = np.concatenate(r50_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQW1bzwkKbIg",
    "outputId": "9319cac3-0ac9-46e4-d8f2-26208efdb8f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test NDCG@100=0.43671 (0.00207)\n",
      "Test Recall@20=0.39788 (0.00261)\n",
      "Test Recall@50=0.53831 (0.00276)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test NDCG@100=%.5f (%.5f)\" % (np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))))\n",
    "print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
    "print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list), np.std(r50_list) / np.sqrt(len(r50_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEkCQugNKbIg"
   },
   "source": [
    "### Train a Multi-DAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOnTW8teKbIg"
   },
   "source": [
    "The generative function is a [200 -> n_items] MLP, thus the overall architecture for the Multi-DAE is [n_items -> 200 -> n_items]. We find this architecture achieves better validation NDCG@100 than the [n_items -> 600 -> 200 -> 600 -> n_items] architecture as used in Multi-VAE^{PR}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "_PwcgFZ6KbIg",
    "outputId": "07a516cc-62eb-4494-ce42-e78e7b094861"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2658139b33d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_items\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'n_items' is not defined"
     ]
    }
   ],
   "source": [
    "p_dims = [200, n_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "UzQ33K6QKbIg",
    "outputId": "b72053c0-a3fc-479c-dc7b-b8ad97cbe06b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d3225163a736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiDAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m98765\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "dae = MultiDAE(p_dims, lam=0.01 / batch_size, random_seed=98765)\n",
    "\n",
    "saver, logits_var, loss_var, train_op_var, merged_var = dae.build_graph()\n",
    "\n",
    "ndcg_var = tf.Variable(0.0)\n",
    "ndcg_dist_var = tf.placeholder(dtype=tf.float64, shape=None)\n",
    "ndcg_summary = tf.summary.scalar('ndcg_at_k_validation', ndcg_var)\n",
    "ndcg_dist_summary = tf.summary.histogram('ndcg_at_k_hist_validation', ndcg_dist_var)\n",
    "merged_valid = tf.summary.merge([ndcg_summary, ndcg_dist_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YkGwe6sKbIg"
   },
   "source": [
    "Set up logging and checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgD1SObKKbIg"
   },
   "outputs": [],
   "source": [
    "arch_str = \"I-%s-I\" % ('-'.join([str(d) for d in dae.dims[1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NeTVNSjiKbIg"
   },
   "outputs": [],
   "source": [
    "log_dir = '/content/drive/My Drive/hegoiy/log/DAE/{}'.format(arch_str)\n",
    "\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "\n",
    "print(\"log directory: %s\" % log_dir)\n",
    "summary_writer = tf.summary.FileWriter(log_dir, graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dwanb3xaKbIh"
   },
   "outputs": [],
   "source": [
    "chkpt_dir = '/content/drive/My Drive/hegoiy/checkpoints/DAE/{}'.format(arch_str)\n",
    "\n",
    "if not os.path.isdir(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir) \n",
    "    \n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJ_zOlxlKbIh"
   },
   "outputs": [],
   "source": [
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "veNCK_HXKbIh"
   },
   "outputs": [],
   "source": [
    "ndcgs_vad = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    best_ndcg = -np.inf\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        np.random.shuffle(idxlist)\n",
    "        # train for one epoch\n",
    "        for bnum, st_idx in enumerate(range(0, N, batch_size)):\n",
    "            end_idx = min(st_idx + batch_size, N)\n",
    "            X = train_data[idxlist[st_idx:end_idx]]\n",
    "            \n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')           \n",
    "            \n",
    "            feed_dict = {dae.input_ph: X, \n",
    "                         dae.keep_prob_ph: 0.5}        \n",
    "            sess.run(train_op_var, feed_dict=feed_dict)\n",
    "\n",
    "            if bnum % 100 == 0:\n",
    "                summary_train = sess.run(merged_var, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_train, global_step=epoch * batches_per_epoch + bnum) \n",
    "                    \n",
    "        # compute validation NDCG\n",
    "        ndcg_dist = []\n",
    "        for bnum, st_idx in enumerate(range(0, N_vad, batch_size_vad)):\n",
    "            end_idx = min(st_idx + batch_size_vad, N_vad)\n",
    "            X = vad_data_tr[idxlist_vad[st_idx:end_idx]]\n",
    "\n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')\n",
    "        \n",
    "            pred_val = sess.run(logits_var, feed_dict={dae.input_ph: X} )\n",
    "            # exclude examples from training and validation (if any)\n",
    "            pred_val[X.nonzero()] = -np.inf\n",
    "            ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, vad_data_te[idxlist_vad[st_idx:end_idx]]))\n",
    "        \n",
    "        ndcg_dist = np.concatenate(ndcg_dist)\n",
    "        ndcg_ = ndcg_dist.mean()\n",
    "        ndcgs_vad.append(ndcg_)\n",
    "        merged_valid_val = sess.run(merged_valid, feed_dict={ndcg_var: ndcg_, ndcg_dist_var: ndcg_dist})\n",
    "        summary_writer.add_summary(merged_valid_val, epoch)\n",
    "\n",
    "        # update the best model (if necessary)\n",
    "        if ndcg_ > best_ndcg:\n",
    "            saver.save(sess, '{}/model'.format(chkpt_dir))\n",
    "            best_ndcg = ndcg_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jckmp0taKbIh"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(ndcgs_vad)\n",
    "plt.ylabel(\"Validation NDCG@100\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzzHzNc2KbIi"
   },
   "source": [
    "### Compute test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K53KyHCgKbIi"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dae = MultiDAE(p_dims, lam=0.01 / batch_size)\n",
    "saver, logits_var, _, _, _ = dae.build_graph()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYBO2gmxKbIi"
   },
   "source": [
    "Load the best performing model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEAJe5t5KbIi",
    "outputId": "f5bc8ea7-31cb-4262-df87-c01f27728f12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chkpt directory: /content/drive/My Drive/hegoiy/checkpoints/DAE/I-200-I\n"
     ]
    }
   ],
   "source": [
    "chkpt_dir = '/content/drive/My Drive/hegoiy/checkpoints/DAE/{}'.format(arch_str)\n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tXTvYM_GKbIi",
    "outputId": "1b577af3-b08e-433f-a316-668ceb73f89b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/hegoiy/checkpoints/DAE/I-200-I/model\n"
     ]
    }
   ],
   "source": [
    "n100_list, r20_list, r50_list = [], [], []\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    saver.restore(sess, '{}/model'.format(chkpt_dir))\n",
    "    \n",
    "    for bnum, st_idx in enumerate(range(0, N_test, batch_size_test)):\n",
    "        end_idx = min(st_idx + batch_size_test, N_test)\n",
    "        X = test_data_tr[idxlist_test[st_idx:end_idx]]\n",
    "\n",
    "        if sparse.isspmatrix(X):\n",
    "            X = X.toarray()\n",
    "        X = X.astype('float32')\n",
    "\n",
    "        pred_val = sess.run(logits_var, feed_dict={dae.input_ph: X})\n",
    "        # exclude examples from training and validation (if any)\n",
    "        pred_val[X.nonzero()] = -np.inf\n",
    "        n100_list.append(NDCG_binary_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=100))\n",
    "        r20_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=20))\n",
    "        r50_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=50))\n",
    "\n",
    "n100_list = np.concatenate(n100_list)\n",
    "r20_list = np.concatenate(r20_list)\n",
    "r50_list = np.concatenate(r50_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJbCzt-JKbIi",
    "outputId": "ecae6122-8ba2-4936-c5b9-2b19a6e066a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test NDCG@100=0.43525 (0.00210)\n",
      "Test Recall@20=0.39381 (0.00260)\n",
      "Test Recall@50=0.52518 (0.00275)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test NDCG@100=%.5f (%.5f)\" % (np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))))\n",
    "print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
    "print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list), np.std(r50_list) / np.sqrt(len(r50_list))))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "jEkCQugNKbIg",
    "wzzHzNc2KbIi"
   ],
   "name": "vae.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
